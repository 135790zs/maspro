NEUROMORPHICS:
- Neuromorphics (Sboev2020/1-3)
- BPTT is implausible for neuromorphic (Bellec1)
- E-prop is better suited for neuromorphic hardware and would provide a qualitative jump in performance (Bellec2019).
- SNNs emerged as an ideal biologically inspired neuromorphics paradigm for realizing energy-efficient on-chip intelligence hardware (Zhou2020/27,28)
- High-level simulation platforms that can integrate memristive device models and their putative non-idealities into crossbar architectures within DL systems (Lammie2020).
- Neuromorphic architectures are more parallel and consume less power (Yu2020/3, Zhang2020/2)
- Neuromorphics are more efficient for machine learning than VN (Yu2020/4, Sboev2020/4)
- Neuromorphics can be used for complex neural network operations (Yu2020/5)
- Neuromorphics can be used in applying deep SNNs in medical applications.
- Neuromorphics have been used for mapless navigation with 75x lower power and slightly better performance (Tang2020).
- Neuromorphics have been used in realizing LIF SNNs in field-programmable gate arrays, obtaining competitive image classification performance with a 6-order of magnitude speedup compared to simulations (Zhang2020).
- Hierarchical neuromorphic network-on-chips have been used to address the scalability issue of SNNs (Zhang2020/6).
- Neuromorphics have been presented that support large-scale conductance-based SNNs (Zhang2020/8,9, Markovic2020/104,105).
- Neuromorphic chips do not incorporate the latest progresses of the field due to manufacturing delays (Markovic2020).
- Neuromorphic Izhikevich neurons on field-programmable gate arrays have yielded better performance than LIF (Zhang2020/12).
- Neuromorphic self-repairing SNNs that mimic biological self-repair can detect up to 40% fault density while maintaining performance (Zhang2020/7).
- Neuromorphic SNNs have been used for low-power solutions for localization and mapping of mobile robots (Tang2020/9), for planning (Tang2020/10) and control (Tang2020/11).
- Neuromorphic SNNs have been used to mimick the BCM-rule as a typical case of STDP (unknown: probably in Wang2020a or Tang2020?).
- Neuromorhic RSNNs have been implemented (Bellec2019/6,7).
- More physics and material science is requires to build efficient neuromorphic architectures (Markovic2020).
- Some cross-fertilization between neuromorphics and quantum computing is starting to take place (Markovic2020/68,69), as quantum superposition and entanglement may be used to process information in parallel and in a high-dimensional state space (Markovic2020/101-103).
- BCM has been demonstrated using rate-based presynaptic spikes in memristors (Wang2020b/34-36).
- Triplet-STDP has been emulated in memristors (Wang2020b/??).
- Energy consumption of CMOS artificial neurons is several orders of magnitude lower than conventional artificial neurons, and 2-3 orders of magnitude lower than biological neurons (Elbez2020).
- Lasers have been used to build efficient and easily implemented STDP-based photonic neuromorphic systems (Wang2020a; Xiang2020).
- Neuromorphics are attracting strong interest because of their massive parallelism, high energy efficiency, good error tolerance, and good ability to implement cognitive functions (Wang2020b/1-6).
- Nanodevices that emulate biological synapses with learning functions can benefit neuromorphics (Wang2020b/7-13), particularly the two-terminal memristor (Wang2020b/14-19).
- More recently, the focus in neuromorphics is low power consumption (Yu2020/5-7)
- Neuromorphic computing typically uses sparse, event-based communication and physically colocalized memory and computation (Zenke2021/29,30)
- Colocalized memory and computation in Von Neumann machines has been implemented in Google's TPU, Graphcore's IPU, and Cerebras' CS-1. (Zenke2021).
- Address event representation allows large networks of VLSI neurons and synapses to communicate asynchronously (Zenke2021/31,32)
- Emulating neural dynamics on a physical substrate is more efficient but requires constraints to match the brain's timescales (Zenke2021/40).
- Feedback connections, for which the brain uses neurotransmitters, may become problematic in large-scale neuromorphic systems.


BIOLOGICAL HEBBIAN/STDP / BRAIN:
- Local clustering of temporally correlated inputs on dendritic branches may counteract the problem of catastrophic forgetting (Limbacher2020).
- Three-factor Hebbian learning (fremaux2016neuromodulated)
- Three-factor Hebbian learning outperforms classical Hebbian learning (porr2007learning)
- The cortex contains neurons with Spike-Frequency Adaptation (Bellec2019/2).
- STDP happens (abbott2000synaptic for review; Zhou2020/29-35; kandel2000principles)
- STDP forms associations between memory traces in SNNs, which are crucial for cognitive brain function (Pokorny2020).
- STDP is a fundamental learning principle in the brain (Traub2020/5)
- STDP requires modulatory signals to happen (bailey2000heterosynaptic)
- STDP happens in primate visual cortex (Traub/8)
- The brain actively maintains traces of spiking activity by e.g. (CaMKII enzymes) (Bellec2019/9).
- If in the brain an eligibility trace is followed by a learning signal, synaptic plasticity occurs (Bellec2019/10-12).
- Neural firing are related (like neurotransmitters) to induction of ERN (Bellec2019/13).
- Homeostatic mechanisms are neccesary for stable biological neuronal and network function (Elbez2020/20) [related to adaptive thresholds].
- Dopamine signaling has a behavioral and functional role in the primary motor cortex (Athalye2020/8-11).
- Dopamine synapse modulation involves dendritic spine enlargement during a very narrow time window, known as reinformence plasticity (yagishita2014critical).
- Dopamine gates neuroplasiticity corticostriatal synapses (legenstein2008/6,7) and within the cortex (Legenstein2008/8)
- Dopamine is behaviorally related to novelty and reward prediction (Legenstein2008/5, li2003dopamine).
- Dopamine learning signals have been found not to be global, but to be specific for local populations of neurons (Bellec2019/14,15).
- Acetylcholine gates synaptic plasticity in the cortex (Legenstein2008/9)
- Suggested that brain solves credit assignment problem by combining local eligibility traces with neuromodulator-based reward signals (Traub2020/6)
- Synaptic connections are pruned during the early stages of human brain development (Elbez2020/8).
- The learning principles of biological NNs are not explored enough to design engineering solutions (gorban2019unreasonable, Zhang2020/3).

IMPLEMENTATIONS:
- R-STDP was used to train a stable lane keeping controller (Bing2020)
- R-STDP was used to recognize action sequences present in a video (source unknown but conclusion of one of the papers).
- Gradient descent has been used to train neuromodulated LSTMs (outperform convertional LSTMs) (Miconi2020).
- Pseudoderivatives have been used to approximate gradient descent in feedforward SNNs (Bellec2019/40-44).
- Pseudoderivatives for spikes have been used in (Bellec2019/45-46).
- Synaptic plasticity was reached by combining learning signals and eligibility traces in feedforward SNNs (Bellec2019/40,42,43). These methods would not be local in RSNNs (see Bellec2019 why). (Bellec2019/40) suggests need to extend their approach to RSNNs.
- Some RSNN training methods rely on control theory to train a chaotic reservoir of neurons (Bellec2019/32-34)
- The FORCE training method can train RSNN but is nonlocal (Bellec2019/35,36).
- There exists an online but nonlocal RNN training method based on gradient descent (williams1989learning).
- Neuromodulated eligibility traces have been experimentally demonstrated to perform Hebbian learning to allow stable learning in a RNN (he2015distinct).
- Phasic rewards at the end of a trial can train chaotic RNNs in tasks that require context-dependent associations and memory maintenance, in neural dynamics that resemble those observed during cognitive tasks (miconi2017biologically).

OTHER/UNKNOWN:
- Three-factor Hebbian learning in RSNN is competitive with BPTT [Bellec1]
- R-STDP can have predictable learning effect. [Legenstein2008]
- Brain is different from deep ANNs <Bellec1>
- The brain is an inherently time-dependent dynamical system (Zenke2021/37,38) that relies on biophysical processes, recurrence, and feedback of its physical substrate for computation (Zenke2021/30,39).
- Von Neumann computers fundamentally different from brain (Yu2020/1)
- Deep Learning is energy inefficient because it suffers from the Von Neumann bottleneck (Zenke2021)
- BPTT is implausible for brain (Bellec2019/5)
- Backpropagation is biologically implausible (Weidel2020/l.26).
- RNNs superior for temporal dimension (Bellec1)
- E-prop better RSNN learning performance than previously known methods [Bellec1]
- R-STDP can lead to both spatial and temporal pattern classification. [Legenstein2008]
- R-STDP can solve difficult credit assignment. [Legenstein2008]
- R-STDP does not endanger stability of network even for long time periods [Legenstein2008]
- RTRL is a computationally expensive and local method that can learn tasks in RNNs by using gradient descent (Bellec2019/8).
- Approximations of RTRL that are less computationally expensive but still nonlocal are described in (Bellec2019/48-51) and only works for supervised regression in generic nonspiking networks.
- The general form of the eligibility trace collects all contributions of the loss gradient that can be locally and online computed (Bellec2019).
- Future work on e-prop could explore a combination with attention-based models in order to cover multiple timescales (Bellec2019).
- Unsupervised learning and clustered connectivity can improve R-STDP (weidel2020).
- Including neurons with SFA improves performance of RNNs (Bellec2019/3)
- It was suggested that re-entering of neural dynamics happens on two time scales (Athalye2020).
- BPTT can be applied to training recurrent SNNs (Traub/1, Bellec2019/3,4).
- STDP types (Vigneron2020).
- BPTT (Traub/14)
- LIF (burkitt2006review)
- SNNs (maass1997networks)
- Triplet-STDP (Wang2020b/33,40-42)
- SNNs with unsupervised STDP can learn spatio-temporal patterns of input signals, especially online (Zhou2020/36-38).
- SNNs with multiple hidden layers can extract more complex features to obtain high classification performance (Zhou2020/39,40).
- Convolutional SNNs with unsupervised STDP outperform CNNs in melanoma skin lesion classification in accuracy, efficiency, and simplicity (Zhou2020).
- Deep SNNs can reach classification performances comparable to CNNs (Zhou2020/41-43).
- SNNs have shown very good performance in visual processing (Zhou2020/41,44,45) and speech recognition (Zhou2020/46,47).
- SNN-based control tasks were successfully performed by manually setting network weights (see Bing2020 p.22 for sources)
- SNNs with STDP can exhibit classical and operant conditioning (Lobov2020).
- STDP can only happen in eligibility-trace based learning when the neuron model provides a negated gradient signal in the case when a presynaptic signal arrives too late (Traub2020)
- E-prop provides a hypothesis for the functional role of the experimentally found (Bellec2019/14) diversity of dopamine signals to different populations of neurons (Bellec2019).
- Random broadcast alignment can perform as effectively as backpropagation in some tasks in feedforward ANNs (Bellec2019/16,17) and multi-layer SNNs (Bellec2019/18).
- Random broadcast alignment has poor performance in deep feedforward networks for complex image classification tasks (bartunov2018assessing)
- Random broadcast alignment has been suggested to provide a diversity of feature detectors for task-relevant network inputs (Bellec2019).
- Eligibility trace proposed by (izhikevich2007solving) and (florian2007reinforcement)
- Eligibility traces can solve a number of interesting learning tasks (izhikevich2007solving, legenstein2008learning, and bellec2020solution)
- E-prop suggests that the experimentally found diverse time constants of the firing activity of populations of neurons in different brain areas (Bellec2019/30) are correlated with their capability to handle corresponding ranges of delays in temporal credit assignment for learning. (Bellec2019)
- By integrating stochastic synaptic rewiring (Bellec2019/24) into sparse LSNNs, short-term memory can emerge (Bellec2019).
- TIMIT is one of the most commonly used benchmarks for temporal processing capabilities of RNNs. (greff2016lstm)
- RSNNs consisting only of LIF neurons do not reach good performance with BPTT (Bellec2019/3).
- A RSNN with mixed LIF and ALIF neurons can reach similar performance using BPTT as a LSTM (Bellec2019/3).
- Larger SNNs are generally needed to obtain a good performance (Elbez2020).
- A compression technique exists that can compress 75% of a SNN while preserving performance (Elbez2020).
- SNN pruning after training can remove 50% of synapses and keep 90% of accuracy (Elbez2020/17).
- SNN pruning during training can remove 75% of synapses and keep 90% of accuracy (Elbez2020/18).
- Attempts have been made to apply backprop to SNNs (hong2010cooperative; xu2013supervised, Sboev2020a/12-14, Weidel2020/l.28)
- With input preprocessing such as Gaussian receptive fields, even simple models such as a single-layer IF neurons achieve an F1-score of 99% in the Fisher's Iris dataset, comparable to MLPs (Sboev2020a).
- R-STDP has been used for mapless navigation but suffers from catastrophic forgetting and lack of policy evaluation (Tang2020/13).
- Initial synaptic weights significantly affect the performance of STDP-based SNNs (Kim2020) (higher initials = better).

