%************************************************
\chapter{Discussion}\label{ch:discussion}
%************************************************

\paragraph{Interpretation of results}
% Effect of STDP
% Firing rate tends to decrease slower for non-STDP; refer back to runaway excitation and demo graph.
The results show that the ALIF and STDP-ALIF neuron perform better than the corrected (and default) Izhikevich e-prop neuron model in classifying phones in the TIMIT dataset.
The addition of the STDP mechanism in the ALIF neuron results in a slower, but stabler and more accurate learning curve.
However, the Izhikevich neuron shows STDP behavior too, suggesting that the STDP-ALIF neuron works by virtue of another factor, such as its spike frequency adaptation which the Izhikevich neuron does not have.

Rearranging the neurons into a stacked architecture does not appear to improve the classification performance.

\paragraph{Possible improvements}
    There are many hypothetical ways of improving the performance or biological plausibility of e-prop that have not yet been considered in this report.
    For instance, a likely reason that the learning speed of the multi-layer architectures was slower than their single-layer counterparts is that the weights are poorly initialized.
    Empirical observation of the learning process suggested that spiking activity faded in deeper layers, because the spiking activity from a preceding layer is generally weaker than the input values the first layer receives.
    Higher weights in-between layers mitigate this fading activity, but require some search to find a good value.
    In this report, the firing rate regularization term approximated this value, but learning is more efficient with a better initialization, since initial synaptic weights significantly affect the performance of STDP-based SNNs \citep{kim2020initial}.

    Also in this report, certain parameters such as firing rate targets ($f^\text{target}$), activity leak ($\alpha$), threshold adaptivity ($\beta$), and feedback signals were constant for all neurons, except the threshold adaptivity, which was 0 for some neurons to emulate non-adaptive LIF neurons.
    Future research could examine the effects of sampling some of these parameters from a distribution for each neuron, thereby creating a more diverse population of neurons with different time scales.
    This sampling requires a careful assessment of the time scales related to the learning task; in particular, the network should be able to process the slowest relevant time scale of the task \citep{jaeger2021dimensions}.
    In the TIMIT classification task, for instance, this could span the whole time fragment, because initial words give semantic context to all subsequent words.
    However, a more predictive time scale for the TIMIT dataset is on the scale of approximately 8 MFCC frames, which is now accurately captured by the adaptive threshold component in the ALIF neuron model.
    Temporal dependencies can also be found when moving in the opposite direction----later words and sounds give informative context to earlier words and sounds.
    In \citet{bellec2020solution}, this context was captured in a bidirectional network, improving the accuracy by nearly 10\%.
    In this report, this was empirically validated as well, but left undiscussed because a bidirectional network is not biologically plausible, as directly accessing earlier input values violates the ``online'' constraint.
    According to \citet{bellec2020solution}, e-prop suggests that the experimentally found diverse time constants of the firing activity of populations of neurons in different brain areas \citep{runyan2017distinct} are correlated with their capability to handle corresponding ranges of delays in temporal credit assignment for learning.
    Setting different values for these parameters per layer might also have a beneficial effect; \citet{ahmed1998estimates} suggested that deeper layers display slower and weaker adaptation rates than shallow layers.

    Random dropout of 80\% of the recurrent network weights led to better training and validation performances.
    This suggests that the effects of the topology within a layer might positively affect the learning process.
    In the brain, neurons tend to connect to nearby neurons.
    A simple lattice topology might better approximate the connectivity of the brain, decrease the computational complexity in both emulations in von Neumann machines, and allow easier on-chip implementations in neuromorphic hardware.
    Hierarchical clustering of neurons might also have a beneficial effect, as this has been demonstrated to improve R-STDP in SNNs \citep{weidel2020unsupervised} and address the scalability issue of SNNs \citep{carrillo2012scalable}.
    Because a neuromorphic system can support complex network operations \citep{hasler1990vlsi}, large-scale conductance-based SNNs \citep{yang2019scalable,Yang2019RealTimeNS} and asynchronous communication in VLSIs through address-event-representation \citep{lazzaro1993silicon,deiss1999pulse} might be suitable to further customize the connectivity graph of an e-prop architecture.

    Other exciting research avenues include connectivity graphs that change over time through a dynamic pruning and growing of weights and neurons.
    Here, the biological motivation is that the human brain prunes synaptic connections during early development \citep{huttenlocher1979synaptic}.
    \citet{elbez2020progressive} demonstrated that 75\% of a SNN can be compressed while preserving its performance, but it is not clear if this can be applied in a biologically plausible way in the e-prop framework.
    However, integrating stochastic synaptic rewiring \citep{kappel2018dynamic} into an ALIF network can improve its short-term memory \citep{bellec2020solution}.

    Finally, synaptic delay might improve the temporal processing power of an e-prop model.
    In this report, communication between neurons was transmitted as a spike over a synapse with a delay of 1 ms.
    This delay could be variable (\eg, between 1 and 10 ms), such that potentially informative past inputs are more accurately preserved in synaptic delays, rather than only in eligibility traces and activity loops.
    This can help deal with tasks that require processing information on multiple time spans \citep{jaeger2021dimensions}.
    This resembles the variable physical length of myelinated biological synapses and number of nodes of Ranvier along them, affecting the conductance of the action potential from one neuron to another \citep{bean2007action}.


\paragraph{Future directions}
    As neuromorphic computing matures, neuroscience improves, and DL increasingly hits fundamental limitations, there is an exciting future for biologically plausible SNNs.
    There is much to gain from cross-fertilization between these fields.
    The popularity of DL was accelerated by accessible platforms to implement and deploy ANNs.
    Similar high-level simulation platforms are now in active development, which can integrate the typical behavior of memristive device models into crossbar architectures within DL systems \citep{lammie2020memtorch}.

    Recent advances in neuromorphic computing indicate this increasing popularity.
    Neuromorphic architectures have been used for mapless navigation with 75 times lower power and better performance \citep{tang2020reinforcement}; as low-power solutions for simultaneous localization and mapping of mobile robots \citep{tang2019spiking}, for planning \citep{fischl2017path}, and control \citep{blum2017neuromorphic}; and self-repairing SNN for fault detection \citep{zhu2017target}.
    While cross-fertilization between neuromorphic computing and quantum computing is starting to take place \citep{russek2016stochastic}, as quantum superposition and entanglement can be used to process information in parallel and in a high-dimensional state space \citep{fujii2017harnessing, yamamoto2017coherent,tacchino2019artificial}, more physics and material science is required to build efficient neuromorphic architectures \citep{markovic2020physics}.
    The same holds for the cross-fertilization between neuroscience and learning rules of biologically plausible SNNs.
    Nanodevices that emulate biological synapses with learning functions can benefit neuromorphic architectures \citep{yao2017face,wang2018photonic,ren2018analytical}, particularly the two-terminal memristor \citep{jo2010nanoscale,wang2017memristors}.
    However, the learning principles of biological NNs are not explored enough to design engineering solutions \citep{gorban2019unreasonable,taherkhani2018supervised}.
    Feedback connections, for which the brain uses neurotransmitters, may become particularly problematic in large-scale neuromorphic systems.
    Another issue in analog computation is how to match the system's internal temporal processing to that of its inputs.
    Emulating neural dynamics on a physical substrate is more efficient but requires constraints to match the brain's timescales \citep{mead1990neuromorphic,jaeger2021dimensions}.
    Future work on e-prop could explore a combination with attention-based models in order to cover multiple timescales \citep{bellec2020solution}.
