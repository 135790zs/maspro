%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

The human brain is one of the most complex systems in the universe.
Its approximately 86 billion neurons \citep{azevedo2009equal} and 100--500 trillion synapses \citep{drachman2005we} are capable of abstract reasoning, pattern recognition, memorization, and sensory experience---while consuming only about \SI{20}{\watt} of power \citep{sokoloff1960metabolism,drubach2000brain}.

An early goal of artificial intelligence has been to construct systems that exhibit similar intelligent traits \citep{turing2009computing}.
One of the proposed methods was to emulate the network of biological neurons in the human brain using simple units named perceptrons \citep{rosenblatt1958perceptron}, inspired by Hebbian learning \citep{hebb1949organization}, which was a new (and later corroborated) theory on biological learning.
This proved to be more difficult than initially hoped, partly due to ostensibly insurmountable problems such as the inability of perceptrons to learn linearly inseparable tasks \citep{minsky1969introduction}.
As a consequence, excitement waned and funding dried up \citep{crevier1993ai}, and the field focused on more practical applications, such as expert systems and symbolic reasoning \citep{haugeland1985artificial}.
After the popularization in the 1980s of trainable Hopfield networks \citep{hopfield1982neural} and backpropagation \citep{rumelhart1986learning}, which enabled learning linearly inseparable tasks, artificial neural networks (ANNs) and the connectionist approach were embraced with a new appreciation.

These ANNs are networks of small computational units that can be trained to perform specific pattern recognition tasks.
Backpropagation has proven to work well in training ANNs with multiple layers, most popularly in the field of deep learning (DL), which has become a dominant field in artificial intelligence.
This popularity is partly due to exponentially increasing computing power and data storage capabilities, and the rise of the Internet has also provided abundant training data.
Some variations on ANNs have shown to improve learning performance, such as using convolutional (CNN) and recurrent (RNN) neural networks, both of which are, like the perceptron, inspired by the architecture of the human brain \citep{fukushima1982neocognitron,lecun1995convolutional,lukovsevivcius2009reservoir}.
These types of networks approach or exceed human level performance in some areas \citep{schmidhuber2015deep}.

\paragraph{Energy limits}
However, DL-based methods are starting to show diminishing returns; training some state-of-the-art models can require so much data and computing power that only a small number of organizations has the resources to train and deploy them.
The computational processes of self-driving cars, for example, consume on the order of a thousand watts.
One of the current top submissions of the Labeled Faces in the Wild (LFW) face verification task is a deep ANN by Paravision was trained using a dataset of 10 million face images of 100 thousand individuals\footnote{See \texttt{http://vis-www.cs.umass.edu/lfw/results.html\#everai}. Last accessed January 2021.}.
Beside very large datasets, deep ANNs also require a significant amount of power to train.
For instance, ResNet \citep{he2016deep} has been trained for 3 weeks on a 8-GPU server consuming about 1 GWh.
A more extreme case is the 11-billion parameter version of Google's T5 model \citep{raffel2019exploring}, which is estimated to cost more than \$1.3 million per training run \citep{sharir2020cost}.
This difference in power consumption precludes computations in mobile low-power or small-scale devices, which now requires at least a connection to a cloud computing server.

The energy consumption of DL contrasts strongly with the that of the human brain, which can learn patterns using far less energy and data.
This is because despite the biologically inspired foundation, deep ANNs are fundamentally different from the brain, which is an inherently time-dependent dynamical system \citep{sacramento2018dendritic, wozniak2020deep} that relies on biophysical processes, recurrence, and feedback of its physical substrate for computation \citep{sterling2015principles,bhalla2014molecular}.
Deep ANNs are implemented on von Neumann architectures \citep{von1993first}, \ie, a system with a central processing unit (CPU) and separate memory, which are significantly different from the working model of the brain \citep{schuman2017survey}.

One reason for the inefficiency of deep ANNs is that they suffer from the von Neumann bottleneck \citep{zenke2021brain}, which involves a limited throughput between the CPU and memory---a data operation cannot physically co-occur with fetching instructions to process that data because they share the same communication system.
Parallelization on GPUs has alleviated this bottleneck to some extent, but the human brain is more efficient as it is embedded in a physical substrate whose neurons operate fully in parallel \citep{a2017parallel} using sparsely occurring postsynaptic potentials (or \emph{spikes}) \citep{bear2020neuroscience}, and where no explicit data processing instructions exist.
Connections in ANNs are represented abstractly by large weight matrices, which are all multiplied with neuron activation values at every propagation cycle.
In the brain, a synapse spikes sparsely and thereby saves energy while conveniently including an informative temporal component.
There are also no matrix multiplications in the brain; a spike can be represented as a binary value which causes the synapse to increase the membrane potential in the efferent neuron to change by a fixed value \citep{bear2020neuroscience}.

A second reason is that backpropagation requires two passes over the ANN: the first to compute the network output given an input, and the second to propagate the output error back into the network to move the weights between neurons in the direction of the negative gradient.
Backpropagation in RNNs is often performed by unrolling the network in a feedforward ANN in a process named backpropagation through time (BPTT).
The human brain, in contrast, is unlikely to use backpropagation, BPTT, or gradients of the output error \citep{lillicrap2019backpropagation}.

\paragraph{Spiking neural networks}
Spiking neural networks (SNNs) \citep{maass1997networks,gerstner2002spiking} are another step towards biological plausibility of connectionist models.
The concept dates back to the 1980s \citep{hopfield1982neural}; nevertheless, they are sometimes considered as the third generation of ANNs \citep{maass1997networks}.
SNNs use neurons that do not relay continuous activation values at every propagation cycle, but spike once when they reach a threshold value.
SNNs are competitive to ANNs in terms of accuracy and computational power, as well in they ability to display precise spike timings \citep{lobo2020spiking}.
Their sparse firing regimes also offer improved interpretability of their behavior as compared to traditional ANNs \cite{soltic2010knowledge}, which is desired in areas such as medicine or aviation.

However, SNNs have not been as popular as ANNs.
One reason for this is that spike-based activation is not differentiable.
As a consequence, backpropagation cannot be directly used to move in the negative direction of the error gradient, although some attempts have been made to bridge this divide \citep{bohte2002error,hong2010cooperative,xu2013supervised,ourdighi2016efficient,lee2016training,sacramento2018dendritic,bellec2019biologically,whittington2019theories} and to make backpropagation more biologically plausible.

Similarly, it has been demonstrated that BPTT can be applied to recurrent SNNs (RSNNs) \citep{huh2017gradient,bellec2018long}.
Some RSNN training methods rely on control theory to train a chaotic reservoir of neurons \citep{thalmeier2016learning,gilra2017predicting}.
Information locality can be preserved in some of there methods \citep{alemi2018learning}, \ie, a neuron or synapse can only access information of itself, or the communication of synapses or neurons (resp.) with which it is directly connected.
The FORCE training method \citep{nicola2017supervised}, in contrast, can also train RSNNs but it nonlocal.
This body of research led to increased understanding of training SNNs and how to obtain better learning performances.
For instance, both single- and multi-layer SNNs have shown good performance in visual processing \citep{escobar2009action,kheradpisheh2018stdp,liu2017fast} and speech recognition \citep{tavanaei2017bio,dong2018unsupervised}.
While DL was rapidly becoming popular during the 2010s, there was no clear learning algorithm for SNNs that could compete with ANNs.
A second reason for the relative unpopularity of SNNs is that they are generally emulated in von Neumann architectures, undermining their energy efficiency advantages.

\paragraph{Neuromorphic computing}  % Gives importance to local & online
SNN learning algorithms are particularly useful in the upcoming discipline of neuromorphic computing (NC) \citep{mitra2008real}, in which analog very-large-scale integration (VLSI) systems are used to implement neural systems.
On the surface, it can be understood as running neural networks not abstracted in a digital system, but physically embedded in an dedicated analog medium.
A central advantage of NC is energy efficiency \citep{hasler1990vlsi,lee1990parallel,tarassenko1990real}.
This energy efficiency, combined with NC's massive parallelism \citep{monroe2014neuromorphic}, makes VLSIs particularly relevant for implementing SNNs.

Like SNNs, neuromorphic systems typically use parse, event-based communication and physically colocalized memory and computation \citep{sterling2015principles,neftci2018data}.
Although colocalized memory and computation has also been implemented in von Neumann machines, such as Google's TPU\footnote{see \texttt{https://cloud.google.com/tpu/docs/tpus}. Last accessed January 2021}, Graphcore's IPU\footnote{see \texttt{https://www.graphcore.ai/products/ipu}. Last accessed January 2021}, or Cerebras' CS-1\footnote{see \texttt{https://cerebras.net/product/\#chip}. Last accessed January 2021}, neuromorphic systems are more efficient for running ANNs \citep{merolla2014million,rajendran2019low}.
The energy consumption of CMOS artificial neurons is several orders of magnitude lower than that of neurons in an ANN, and even 2--3 times lower than the energy consumption of biological neurons \citep{elbez2020progressive}.
Neuromorphic systems are also more tolerant to device variation \citep{yu2013low}.

Because of their massive parallelism, high energy efficiency, good error tolerance, and good ability to implement cognitive functions, neuromorphic systems are attracting strong interest.
In particular, SNNs emerged as an ideal biologically inspired NC paradigm for realizing energy-efficient on-chip intelligence hardware \citep{merolla2014million,davies2018loihi}, suitable for running fast and complex SNNs on low-power devices.
For instance, a competitive image classification performance was reached with a 6-order of magnitude speedup in a leaky integrate-and-fire (LIF) SNN in field-programmable gate arrays, compared to digital simulations \citep{zhang2020low}.

\paragraph{Biological learning}
To run an SNN on neuromorphic hardware, a \emph{local} and \emph{online} algorithm is needed.
The precondition of locality refers to the idea that a neuron or synapse can only access information or communication with which it is physically connected.
For instance, the inner state of a neuron can only be influenced by itself, or by the spikes it receives from afferent neurons.
Similarly, a synapse can only spike or change its weight based on signals from the afferent and efferent neuron.
This is a direct consequence of the colocalization of processing and memory.
The precondition of online can be regarded as temporal locality---neurons and synapses can only access information that physically exists at the same point in time.
They cannot access future information, nor past information if it was not explicitly retained.

The brain also adheres to these two constraints.
Most learning algorithms for SNNs are rooted in a form of Hebbian learning, which is a major factor in biological learning and memory consolidation.
Classical Hebbian learning is often summarized by ``cells that fire together, wire together'', if there is a causal relationship between these cells, such as a postsynaptic potential on a connecting synapse.
Direct application of Hebbian learning in a spiking neural network will generally lead to a positive feedback loop, because ``wiring cells together'', or increasing the synaptic strength, will in turn increase the likelihood they they also fire together \citep{zenke2017temporal}.
Furthermore, classical Hebbian learning describes no way for a synapse to weaken.

Spike-timing-dependent plasticity (STDP) \citep{abbott2000synaptic,caporale2008spike} is a type of Hebbian learning that incorporates temporal causality: if neuron $B$ spikes right after neuron $A$, then the synapse is strengthened.
If $B$ spikes right before $A$, it is weakened.
However, this too leads to runaway activity through the same positive feedback loop.
It is widely known that STDP is a fundamental learning principle in the human brain \citep{kandel2000principles,caporale2008spike}, including perceptual systems in the sensory cortex \citep{huang2014associative}.
STDP by itself can be used as an unsupervised learning algorithm or to forming associations in classical conditioning \citep{diehl2015unsupervised,kim2018demonstration}.
Furthermore, it has been demonstrated to form associations between memory traces in SNNs, which are crucial for cognitive brain function \citep{pokorny2020stdp}.
To allow supervised learning, or operant conditioning, a learning signal is required to influence the direction of the synapse weight change: a positive learning signal will reinforce the association (long-term potentiation), and a negative learning signal weakens it (long term depression) \citep{lobov2020spatial}.
STDP with a learning signal is known as reward-modulated STDP (R-STDP) \citep{legenstein2008learning} in the field of SNNs and three-factor Hebbian learning in neuroscience \citep{fremaux2016neuromodulated}, outperforming its classical two-factor counterpart \citep{porr2007learning} because learning signals are crucial for maintaining long-term memory \citep{bailey2000heterosynaptic}.

Neurotransmitters are used to modulate the learning signal in the brain.
Dopamine, for instance, which has a central behavioral and functional role in the primary motor cortex \citep{barnes2005activity,dang2006disrupted}, has been shown to modulate synapses through dendritic spine enlargement during a very narrow time window \citep{dang2006disrupted}.
It is behaviorally related to novelty and reward prediction \citep{li2003dopamine,schultz2007behavioral} by gating neuroplasticity of corticostriatal \citep{reynolds2001cellular,reynolds2002dopamine} and ventral tegmental (VTA) synapses \citep{bao2001cortical}.
In the VTA, dopaminergic neurons respond to learning signals in a highly localized manner that is specific for local populations of neurons \citep{engelhard2019specialized}.
This is also the case in other areas of the midbrain \citep{roeper2013dissecting}.
Acetylcholine is another example of a neuromodulator that gates synaptic plasticity in the cortex and enables state-dependent learning, in which memories are recalled better if the sensory context is similar to that during the memory encoding \citep{shulz2000neuronal}.

However, R-STDP by itself does not solve the credit assignment problem, which relates to neuromodulation of synapses after a learning signal is presented with some delay.
In that case, when the learning signal is presented, the neurons have long spiked, and it is not clear which synapses elicited the behavior that is rewarded or punished.
Recent research suggests that the brain uses \emph{eligibility traces} \citep{izhikevich2007solving,florian2007reinforcement} to solve the credit assignment problem \citep{stolyarova2018solving,gerstner2018eligibility}.
An eligibility trace of a synapse is elicited when a neuron spikes, and fades away slowly enough that a delayed learning signal can still modulate the synaptic plasticity through R-STDP \citep{cassenaer2012conditional,yagishita2014critical,gerstner2018eligibility}.
Synaptic plasticity was demonstrated using synaptic plasticity in deep feedforward SNNs \citep{zenke2018superspike,neftci2017event,kaiser2020synaptic} and could be implemented in feedforward VLSIs, but \cite{zenke2018superspike} notes that these methods are applicable for RSNNs.
Eligibility traces have also been shown to solve difficult credit assignment problems in SNNs using R-STDP \citep{legenstein2008learning, bellec2020solution} and in RNNs \citep{he2015distinct}, and have a predictable learning effect \citep{legenstein2008learning}.
R-STDP in SNNs has been used to solve a number of tasks, including training a stable lane keeping controller \citep{bing2020indirect}, but can suffer from catastrophic forgetting and lack of policy evaluation in mapless navogation \citep{bing2018end}.

\paragraph{Eligibility propagation}
Eligibility propagation (e-prop) \citep{bellec2020solution} is a local and online learning algorithm for RSNNs that can be mathematically derived as an approximation to BPTT (see also Section \ref{sec:derivefromBPTT}).
It uses local learning signals and eligibility traces for any type of neuron.
In e-prop, the learning signal is a local variation on random broadcast alignment, which propagates the error directly back onto the neurons with a random weight, resembling the function of a neuromodulator in the brain.
This has been suggested to provide a diversity of feature detectors for task-relevant network inputs \citep{bellec2020solution}.
Broadcast alignment can perform as effectively as backpropagation in some tasks in feedforward ANNs \citep{lillicrap2016random,nokland2016direct} and multi-layer SNNs \citep{samadi2017deep,clopath2010connectivity}, but performs poorly in deep feedforward ANNs for complex image classification tasks \citep{bartunov2018assessing}.

The local and online properties of e-prop make it a biologically plausible learning algorithm that can be implemented on VLSIs.
E-prop has been demonstrated to work for a large variety of tasks, including on classifying phones (\ie, speech sounds), for which it performs competitively with LSTMs, a popular RNN architecture that uses BPTT.

So far, only the LIF and adaptive LIF (ALIF) neuron models have been used in e-prop. In \citet{traub2020learning}, a functional modification was made to the LIF model such that STDP can occur.
In particular, STDP occurs when the neuron model provides a negated gradient signal in the case when a presynaptic signal arrives too late.
This resembles the biological phenomenon of error-related negativity (ERN) \citep{nieuwenhuis2001error}, which is a negative brain response that immediately follows an erroneous behavioral response and peaks after 80--150 ms with an amplitude that depends on the intent and motivation of a person.
\citet{traub2020learning} also showed this effect for the Izhikevich neuron \citep{izhikevich2003simple}.
However, these STDP-modified neurons were shown only in a single-synapse demo to illustrate the STDP properties, not in a full learning task.

\paragraph{Multi-layer RSNNs}
The discovery of backpropagation allowed gradient descent--based training of multi-layer ANNs, which significantly increased their performance.
Multi-layer CNNs show higher levels of abstraction in deeper layers of the network.
For instance, early convolutional filters identify lines and edges, while deeper filters identify more complex shapes.
In RNNs, stacking recurrent layers results in a similar abstraction---but it is temporal instead of spatial.
Deeper layers exhibit slower time dynamics than shallow layers, suggesting that they ignore small variations in the input signal and integrate larger temporal patterns.
It is unclear if these findings extrapolate to RSNNs.

\paragraph{This research}
In this report, the performance of e-prop in \citet{bellec2020solution} is reproduced.
Whereas in \citet{bellec2020solution} e-prop was implemented using automatic differentiation, in this report it is done explicitly, using the e-prop learning equations.
Furthermore, the two new STDP neuron models of \citet{traub2020learning} are experimentally verified in the TIMIT phone classification task.
One of these models, the LIF-STDP, will be modified to an adaptive version named ALIF-STDP.
Finally, the effect of stacking multiple RSNN layers in an e-prop with each of the three neuron models is examined.

Chapter \ref{ch:relatedwork} describes the basic version of the e-prop framework.
Chapter \ref{ch:method} describes the method used to implement the TIMIT learning task and modify the e-prop algorithm to a multi-layer framework with different neuron models.
The results are presented in Chapter \ref{ch:results} and discussed in Chapter \ref{ch:discussion}.
Finally, Chapter \ref{ch:conclusion} concludes this report.
