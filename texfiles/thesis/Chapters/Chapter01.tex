%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

A primary goal of artificial intelligence is to develop systems that exhibit intelligent behavior.
During the 1980s, with the popularization of backpropagation \citep{rumelhart1986learning} and trainable Hopfield networks \citep{hopfield1982neural}, the focus of the field shifted from expert systems and symbolic reasoning to \emph{connectionist} approaches, such as artificial neural networks (ANNs).
ANNs are networks of small computational units that can be trained to perform specific pattern recognition tasks.
These networks are based loosely on the human brain.

As computing power and data storage capabilities increased exponentionally, and the rise of the internet provided abundant training data, ANNs have become a dominant field in artificial intelligence in the context of deep learning (DL).
This has particularly been the case during the the 2010s, when GPUs were increasingly used to train deep neural networks.
During the same period, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) approached or exceeded human level performance in some areas \citep{schmidhuber2015deep}.
CNNs were also inspired by neuroscience; the connectivity pattern between units in a CNN resembles the organization of the primate visual cortex \citep{hubel1968receptive}.

However, DL-based methods are starting to show diminishing returns; training some state-of-the-art models can require so much data and computing power that only a small number of organizations has the resources to train and deploy them.
The computational processes of self-driving cars, for example, consume on the order of a thousand watts.
IFLYTEK-CV, which is one of the best-performing systems in the LFW challenge for facial recognition, was trained using a dataset of 3.8 million face images of 85 thousant individuals.\todo{from vigneron2020, but cite original}
ResNet\todo{cite} has been trained for 3 weeks on a 8-GPU server \todo{specify} consuming about 1 GWh.\todo{from vigneron2020, but cite original}
For a more extreme example, training the 11-billion parameter version of Google's T5 model \citep{raffel2019exploring} is estimated to cost more than \$1.3 million per run\citep{sharir2020cost}.
This contrasts strongly with the energy consumption of the human brain, which is made up of around 86 billion neurons \citep{azevedo2009equal} and 100--500 trillion synapses \citep{drachman2005we}, consumes approximately \SI{20}{\watt} \citep{sokoloff1960metabolism,drubach2000brain}, and does not require as much data to learn patterns.
This difference in power consumption is crucial for computations in mobile low-power or small-scale devices.

One reason why the human brain is more energy-efficient is that its computational function is realized in an analog and massively parallel physical substrate \citep{a2017parallel}, in which neurons communicate through sparsely occurring spikes \citep{bear2020neuroscience}.
Connections in DL models, on the other hand, are represented by multiplications between the often large matrices of the floating-point weight values of these connections and the activation values of the efferent units \citep{lecun2015deep}.
Backpropagation, which has become the de-facto standard for training DL models, is a biologically implausible learning algorithm that trains models by propagating the error back into the network, further raising computational costs.

Spiking neural networks (SNNs) are another step towards biological plausibility of connectionist models.
SNNs use neurons that do not relay continous activation values at every propagation cycle, but spike once when they reach a threshold value.
The concept of SNNs dates back to the 1980s \citep{hopfield1982neural}, but since spike-based activation is differentiable, gradient descent is not as effective as in ANNs to minimize the loss.
Consequentially, the lack of suitable training methods has limited the popularity of SNNs.

Neuromorphic computing is an emerging technology that, like the human brain, peforms computation in a physical subtrate.
This technology has the potential to offer more energy-efficient computation than the von Neumann architecture that is standard in training DL models.
Due to the centralized nature of von Neumann computers, simulated SNNs do not enjoy the energy efficiency as networks of biological neurons.
In theory, however, the massively parallel and decentralized neuromorphic computers can efficiently run SNNs.
This requires a learning algorithm that is both spatially and temporally local (\ie, neurons and synapses can only change their state based on information that is available at the same timestep and immediately adjacent to that neuron or synapse).

This report examines functional modifications to \emph{eligibility propagation} (e-prop), which is a spatially and temporally local learning algorithm for SNNs that suggests a promising approach to train SNNs in a neuromorphic architecture \citep{bellec2020solution}.

\begin{tcolorbox}[colback=orange]
- Brief historical overview of 3F-Hebbian (use my own literature trace)
    - Explain 3F-Hebbian (mention bioplausibility: online \& local)



\end{tcolorbox}

\begin{tcolorbox}[colback=orange]

- E-prop approximates BPTT using RSNNs by using eligibility traces and learning signals. Also mathematical link (only intuition!)


\end{tcolorbox}

\begin{tcolorbox}[colback=orange]

- This paper examines the effects of enhancements that may improve the performance of e-prop. Some of these are used succesfully in DL. (Argue scientifically why these might improve performance)
    - Multilayer. Mention how MLPs were breakthrough on perceptrons.
    - Other neuron types
    - Regularization that's also observed in brain (e.g. synaptic scaling)


\end{tcolorbox}
