%************************************************
\chapter{Theoretical Framework}\label{ch:relatedwork}
%************************************************

\section{Eligibility Propagation}

    \subsection{E-prop model}
        In \citet{bellec2020solution}, an eligibility propagation (e-prop) model $\mathcal{M}$ of a neuron $j$ in a feedforward or recurrent network is defined by a tuple $\left<M, f\right>$,
        where $M$ is a function
        \begin{equation}\label{eq:model}
        \mathbf{h}^t_j = M\left(\mathbf{h}_j^{t-1}, \mathbf{z}^{t-1}, \mathbf{x}^t, \mathbf{W}_j\right)
        \end{equation}
        that defines the hidden state $\mathbf{h}_j^t$ at a discrete time step $t$, where $\mathbf{z}^{t-1}$ is the observable state of all neurons at the previous time step (\ie, the binary spike values), $\mathbf{x}^t$ is the model input vector at time $t$, and $\mathbf{W}_j$ is the weight vector of afferent (\ie, ``incoming'') synapses.
        The hidden state of a neuron contains all variables that are used for a specific neuron type, \eg, an activation value, or a variable that models a neuron's refractory period after a spike.
        In short, Equation \ref{eq:model} indicates that the hidden state is affected primarily by spikes of other neurons $\mathbf{z}^{t-1}$, and the current input to the model $\mathbf{x}^t$, which are both weighted by trainable network weights $\mathbf{W}^\text{rec}_j \subset \mathbf{W}_j$ and $\mathbf{W}^\text{in}_j \subset \mathbf{W}_j$, respectively.

        The function $f$ in $\mathcal{M}$ describes the update of the observable state of a neuron $j$ at time $t$:
        \begin{equation}\label{eq:observable}
        z^t_j = f\left(\mathbf{h}_j^t\right),
        \end{equation}
        such that the spikes elicited by a neuron only depend on its hidden state.
        For instance, a neuron $j$ may spike at time $t$ (\ie, $z^t_j = 1$) if its activity, which is contained in the hidden state, reaches a threshold value.

        The purpose of an e-prop model is that it can be trained to perform a classification task.
        As described in the remainder of this chapter, the weight matrix $\mathbf{W}$ is trained such that the input vectors $\mathbf{x}^t$ yield a good prediction of the class it belongs to.

        An important property of e-prop is that the weight update depends only on the hidden state of an afferent neuron.
        The formalizations described in Equations \ref{eq:model} and \ref{eq:observable} therefore indicate that e-prop is a \emph{local} training method, because a neuron's observable state depends only on its own hidden state, which in turn depends only on observable signals that are directly connected to it.
        E-prop is also an \emph{online} training method, because both the hidden and observable state of a neuron depend only on information that is still available; the observable state is updated at the same time step as the hidden state, and the hidden state is updated according to information which is then present in the afferent neurons.

        The precise formulations of $M$ and $f$ depend on the neuron models that are used in the e-prop model.

    \subsection{Neuron models}\label{sec:alif}

        \paragraph{LIF neuron}
            In \citet{bellec2020solution}, the LIF neuron model is formulated in the context of e-prop, along with a variant (viz. ALIF) that has an adaptive threshold based on the neuron's spiking frequency.
            The observable state of a LIF model is given by
            \begin{equation}\label{eq:heavisideLIF}
            z^t_j = H\left(v_j^t-v_\text{th}\right),
            \end{equation}
            where $H$ is the Heaviside step function, $v^t_j$ is the activity of neuron $j$ at discrete time $t$, and $v_\text{th}$ is the threshold constant.
            (Note that this and all other used hyperparameters are listed in Table \ref{tab:hparams}.)
            From Equation \ref{eq:heavisideLIF} it follows that a neuron spikes ($z^t_j = 1$) if its activity reaches the activity threshold, and remains silent ($z^t_j = 0$) otherwise.
            These spikes are the only communication between neurons in the e-prop model.

            The hidden state $h^t_j$ of a LIF neuron model contains only an activity value $v^t_j$ that evolves over time according to the equation
            \begin{equation}\label{eq:alifV}
            v^{t+1}_j = \alpha v_j^t + \sum_{i\neq j}W^\text{rec}_{ji}z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} - z_j^tv_
            \text{th},
            \end{equation}
            where $W^\text{rec}_{ji}$ is a synapse weight from neuron $i$ to neuron $j$, and $\alpha$ is a constant decay factor.
            In Equation \ref{eq:alifV}, the first term models the decay of the activity value over time.
            The second and third term model the input of the neuron from other neurons, or from the input to the network, respectively.
            The fourth term ($-z^t_jv_\text{th}$) ensures that the activity of the neuron drops when it spikes.
            Furthermore, $z^t_j$ is explicitly fixed to 0 for $T^\text{refr}$ time steps after a spike to model neuronal refractoriness.

            In biological neurons, the refractory period consists of an ``absolute'' phase, during which eliciting a new spike is impossible, and a subsequent ``relative'' phase, during which the threshold is temporarily increased \citep{purves2008neuroscience}.
            Clamping $z^t_j$ to 0 emulates only this absolute phase, and is therefore only a crude approximation to model biological refractoriness.
            The refractory period is built into the equations of the Izhikevich neuron model described in Section \ref{sec:izhikevich}, which is therefore arguably a more biologically plausible neuron model.


        \paragraph{ALIF neuron}
            The ALIF neuron model introduces a threshold adaptation variable $a^t_j$ to the hidden state of the LIF neuron, such that $\mathbf{h}^t_j \eqdef \left[v^t_j, a^t_j\right]$.
            In an ALIF neuron, the spiking threshold increases after a spike, and otherwise decreases back to a baseline threshold $v_\text{th}$ in the continued absence of spikes.

            This resembles \emph{spike frequency adaptation} (SFA), a common feature of neocortical pyramidal neurons \citep{benda2003universal}.
            SFA is a homeostatic control mechanism that affects the spiking frequency based on the recent spiking activity, such that neurons that spike relatively infrequently become more sensitive, and vice versa.
            \citet{ahmed1998estimates} found that a single time constant is a good fit to characterize the threshold's exponential decay to a steady state.

            The observable state of an ALIF neuron is therefore described by
            \begin{equation}\label{eq:alifZ}
            z^t_j = H\left(v_j^t - v_\text{th} - \beta a^t_j\right)
            \end{equation}
            and
            \begin{equation}\label{eq:alifA}
            a^{t+1}_j = \rho a^t_j + z^t_j,
            \end{equation}
            where $\rho < 1$ is an adaptation decay constant and $\beta \leq 0$ is an adaptation strength constant.
            Equation \ref{eq:alifA} indicates that the adaptive threshold increases at every spike, and decays back to $v_\text{th}$ in the absence of spikes.
            The decay factor $\rho$ of the threshold adaptation is higher than the decay factor $\alpha$ of the neuron activity, such that the immediate firing behavior of a neuron is affected on a shorter time scale than the threshold adaptation, which is better suited to reflect the working memory of a neuron and track longer temporal dependencies in the input data then the activation decay.
            The interaction between the neuron activity, adaptive threshold and spiking behavior is illustrated in Figure \ref{fig:simplealif}.

            \begin{figure}[!ht]
                \centering
                \includegraphics[width=\linewidth]{gfx/simplealif}
                \caption[ALIF neuron simulation]{A simulated ALIF neuron $j$ receives a sinusoidal input $I$ for 300 time steps $t$. This figure illustrates the adaptive threshold $a$, which increases at every spike $z$, requiring a higher activity $v$ for a next spike. When a spike occurs, $v$ decreases by $v_\text{th}$. Note that the first wave of the sinusoid elicits a stronger spike train than subsequent waves, demonstrating the homeostatic effect of the adaptive threshold. Note also that on a short time scale, spikes tend to occur primarily in the upward phases of the sinusoid, suggesting that ALIF neurons are well-suited to respond to changes in the input signals.}
                \label{fig:simplealif}
            \end{figure}

            The LIF neuron is a spacial case of an ALIF neuron, for which $\beta=0$, effectively canceling the effect of the threshold adaptation value $a^t_j$ on the observable state $z^t_j$ in Equation \ref{eq:alifZ}.
            Therefore, only the e-prop derivations for the ALIF neurons will be described in the following sections.
            From this point, references to LIF neurons in this report will refer to this special case.

    \subsection{Network topology}
        % First describe network topology, broadly
        % Say that you're experimenting with different topologies (specificially mention that the NN is recurrent)
        The e-prop network structure as used in this report consists of the following main components:
        \begin{enumerate}
            \item An input layer $\mathbf{x}^t$.
            \item A recurrent layer containing $N$ neurons that are connected to all other neurons in this layer by weights $\mathbf{W}^\text{rec}$. This layer is also connected to the input layer by weights $\mathbf{W}^\text{in}$.
            \item An output layer $\mathbf{y}^t$ connected to the recurrent layer by weights $\mathbf{W}^\text{out}$.
        \end{enumerate}

        Since one of the goals of this report is to evaluate multi-layer topologies, the recurrent layer component is modified in Section \ref{sec:ml_arch} to support architectures with a feedforward series of recurrent layers.

        An input vector $\mathbf{x}^t$ at time step $t$ is fed to a pool of $N$ recurrent neurons with hidden states $\mathbf{h}^t$ and observable states $\mathbf{z}^t$ through input weights $\mathbf{W}^\text{in}$.
        The recurrent weights $\mathbf{W}^\text{rec}$ connect neurons with each other, but no self-loops exist.
        Therefore, the recurrent neurons also receive inputs from the observable states of the afferent neurons.
        25\% of these neurons is a LIF neuron (\ie, $\beta = 0$) and the others are ALIF neurons.
        The output weights $\mathbf{W}^\text{out}$ process the observable states of the neurons through a softmax function into a logits layer $\mathbf{\pi}^t$.
        These logits are compared with the target output $\mathbf{\pi}^{*, t}$ and multiplied with broadcast weights $\mathbf{B}^t$ to obtain a learning signal $L_j^t$ for every neuron $j$ in the pool.
        Figure \ref{fig:topology-sl} illustrates the basic architecture of a single-layer e-prop model.
        \begin{figure}[!ht]
            \myfloatalign
            \includegraphics[trim=0 25cm 0 0, clip, width=\linewidth]{gfx/Singlelayer}\caption[Single-layer architecture illustration]{A basic illustration of a single-layer network architecture.
            An input vector $\mathbf{x}^t$ at time step $t$ is fed to a pool of $N$ recurrent neurons with hidden states $\mathbf{h}^t$ and observable states $\mathbf{z}^t$ through input weights $\mathbf{W}^\text{in}$. The recurrent weights $\mathbf{W}^\text{rec}$ connect neurons with each other, but no self-loops exist. A randomly selected 25\% of these neurons is a LIF neuron (\ie, $\beta = 0$) and the others are ALIF neurons. The output weights $\mathbf{W}^\text{out}$ process the observable states of the neurons through a softmax function into a logits layer $\mathbf{\pi}^t$. These logits are compared with the target output $\mathbf{\pi}^{*, t}$ and multiplied with broadcast weights $\mathbf{B}^t$ to obtain a learning signal $L_j^t$ for every neuron $j$ in the pool. Note that weights illustrated in red are e-prop weights, \ie, they track eligibility traces.}
            \label{fig:topology-sl}
          \end{figure}

        Like in \citet{bellec2020solution}, weights are initialized by sampling them from a Gaussian distribution $\mathcal{N}\left(0, \sqrt{N}\right)$ where $N$ is the number of afferent neurons.
        For instance, the weights $\mathbf{W}^\text{in}$ between the input and the first layer are sampled from $\mathcal{N}\left(0, \sqrt{39}\right)$ if there are 39 input features.
        Likewise, each of the neurons has $N-1$ afferent recurrent weights, so the recurrent weights within a layer are sampled from $\mathcal{N}\left(0, \sqrt{N-1}\right)$.

        A randomly selected 80\% of synaptic weights is then set to a value of 0, as well as the synapses that connect a neuron to itself, rendering them ineffective.


    \subsection{Deriving e-prop from RNNs}\label{sec:derivefromBPTT}
        Eligibility propagation is a local and online training method that can be derived from backpropagation through time (BPTT).
        In BPTT, an RNN is unfolded in time, such that the backpropagation method used in feedforward neural networks can be applied to compute the gradients of the cost with respect to the network weights.

        In this subsection, the main equation of e-prop
        \begin{equation}
        \frac{dE}{dW_{ji}} =
        \sum_t\frac{dE}{dz_j^t}\cdot\left[\frac{dz_j^t}{dW_{ji}}\right]_\text{local},
        \end{equation}
        where $\cdot$ denotes the dot product, is derived from the classical factorization of the loss gradients in an unfolded RNN as in \citet{bellec2020solution}:
        \begin{equation}\label{eq:clafac}
        \frac{dE}{dW_{ji}} = \sum_{t' \leq T}\frac{dE}{d\mathbf{h}_j^{t'}}\cdot\frac{\partial \mathbf{h}_j^{t'}}{\partial W_{ji}},
        \end{equation}
        where summation indicates that weights are shared.
        Recall that for ALIF neurons, $\mathbf{h}^t_j \eqdef \left[v^t_j, a^t_j\right]$.
        This is also the true for ALIF neurons that use $\beta=0$ to disable their threshold adaptability.

        By applying the chain rule, the first factor $\frac{dE}{d\mathbf{h}_j^{t'}}$ can be decomposed into a series of learning signals $L_j^t = \frac{dE}{dz_j^t}$ and local factors $\frac{\partial\mathbf{h}_j^{t-t'}}{\partial\mathbf{h}_j^t}$ for all $t$ starting from the event horizon $t'$, which is the oldest time step that information is used from:

        \begin{equation}\label{eq:rec}
        \frac{dE}{d\mathbf{h}_j^{t'}} = \underbrace{\frac{dE}{dz_j^{t'}}}_{L^{t'}_j} \frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \frac{dE}{d\mathbf{h}_j^{t'+1}}\frac{\partial\mathbf{h}_j^{t'+1}}{\partial\mathbf{h}_j^{t'}}.
        \end{equation}
        Note that this equation is recursive.
        If Equation \ref{eq:rec} is substituted into the classical factorization (Equation \ref{eq:clafac}), the full history of the synapse $i\rightarrow j$ is integrated, and a recursive expansion is obtained that has $\frac{dE}{d\mathbf{h}^{T+1}_j}$ as its terminating case:
        \begin{align}
        \frac{dE}{dW_{ji}} &= \sum_{t'}\left(L_j^{t'}\frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \frac{dE}{d\mathbf{h}_j^{t'+1}}\frac{\partial\mathbf{h}_j^{t'+1}}{\partial\mathbf{h}_j^{t'}}\right)\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}\\
        &= \sum_{t'}\left(L_j^{t'}\frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \left( L^{t'+1}_j \frac{\partial z_j^{t'+1}}{\partial\mathbf{h}_j^{t'+1}} + (\cdots)\frac{\partial\mathbf{h}_j^{t'+2}}{\partial\mathbf{h}_j^{t'+1}}  \right) \frac{\partial\mathbf{h}_j^{t'+1}}{\partial\mathbf{h}_j^{t'}}\right)\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}.
        \end{align}

        The recursive parenthesized factor can be written into a second factor indexed by $t$:
        \begin{equation}
        \frac{dE}{dW_{ji}} = \sum_{t'}\sum_{t\geq t'}L^t_j\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}.
        \end{equation}

        By exchanging the summation indices, the learning signal $L_j^t$ is pulled out from the inner summation.

        Within the inner summation, the terms $\frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^t}$ are collected in an \emph{eligibility vector} $\bm{\epsilon}^t_{ji}$ and multiplied with the learning signal $L^t_j$ at every time step $t$.
        This is crucial for understanding why e-prop is an online training method---local gradients are computed based on traces that are directly accessible at the current time step $t$, and the eligibility vector operates as a recursively updated ``memory'' to track previous local hidden state derivatives:

        \begin{equation}
        \bm{\epsilon}^t_{ji} = \frac{\partial\mathbf{h}_j^{t}}{\partial\mathbf{h}_j^{t-1}}\cdot\bm{\epsilon}^{t-1}_{ji} + \frac{\partial\mathbf{h}^t_j}{\partial W_{ji}}.
        \end{equation}

        This is why the $\rho$ and $\alpha$ parameters, which define the decay rate in hidden states and the corresponding eligibility vectors, should be set according to the required working memory in the learning task.
        The eligibility vector and the hidden state have the same dimension: $\left\{\bm{\epsilon}^t_{ji}, \mathbf{h}^t_j\right\} \subset \mathbb{R}^d$, where $d=2$ for the ALIF and Izhikevich neuron models.

        The \emph{eligibility trace} $e^t_{ji}$ is a product of $\frac{\partial z_j^t}{\partial \mathbf{h}_j^t}$ and the eligibility vector, resulting in the gradient that can be immediately applied at every time step $t$, or accumulated and integrated locally on a synapse (see Section \ref{sec:eprop_grd} for details):
        \begin{equation}\label{eq:main_eprop}
        \frac{dE}{dW_{ji}} = \sum_t\frac{dE}{dz_j^t}\underbrace{\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\underbrace{\sum_{t\geq t'}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}}_{\bm{\epsilon}_{ji}^t}}_{e^t_{ji}}.
        \end{equation}
        This is the main e-prop equation.

    \subsection{Learning procedure}

        The e-prop equation (Equation \ref{eq:main_eprop}) can be applied to any neuron type with any number of hidden states.
        In this section, the derivation for ALIF neurons will be detailed.

        \subsubsection{Eligibility trace}
            Recall the hidden state update equations from Section \ref{sec:alif}:
            \begin{equation*}
            v^{t+1}_j = \alpha v_j^t + \sum_{i\neq j}W^\text{rec}_{ji}z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} - z_j^tv_
            \text{th} \tag{\ref{eq:alifV} revisited}
            \end{equation*}
            and
            \begin{equation*}
            a^{t+1}_j = \rho a^t_j + z^t_j \tag{\ref{eq:alifA} revisited}
            \end{equation*}
            and the update of the observable state

            \begin{equation*}
            z^t_j = H\left(v_j^t - v_\text{th} - \beta a^t_j\right). \tag{\ref{eq:alifZ} revisited}
            \end{equation*}

            The hidden state $\mathbf{h}^t_j$ of an ALIF neuron $j$ is therefore a vector containing its activation and threshold adaptation:
            \begin{equation}
            \mathbf{h}^t_j = \begin{pmatrix}
            v^t_j\\
            a^t_j
            \end{pmatrix}.
            \end{equation}
            This hidden state is associated with a two-dimensional eligibility vector
            \begin{equation}
            \bm{\epsilon}^t_{ji} \eqdef \begin{pmatrix}
            \epsilon_{ji, v}^t\\
            \epsilon_{ji, a}^t
            \end{pmatrix}
            \end{equation}
            that relates to a synapse from any afferent neuron $i$ to neuron $j$.

            Recall from Chapter \ref{ch:introduction} that the eligibility trace slowly fades after a spike has occurred on a synapse, such that a delayed learning signal can still modify the synaptic strength accordingly, solving the credit assignment problem.
            Intuitively, the eligibility vector computes the correct contribution of each of the components of the hidden state.
            For a LIF neuron, the only component is the activation value, and so it is simply a low-pass filter of the spikes of the afferent neuron.

            For the default ALIF neuron, however, the hidden state derivative $\frac{\partial\mathbf{h}^{t+1}_j}{\partial\mathbf{h}^t_j}$ must be computed to derive the eligibility vector.
            This hidden state derivative is expressed by a $2\times2$ matrix of partial hidden state derivatives:
            \begin{equation}
            \frac{\partial\mathbf{h}^{t+1}_j}{\partial\mathbf{h}^t_j} = \begin{pmatrix}
            \frac{\partial v^{t+1}_j}{\partial v^t_j} & \frac{\partial v^{t+1}_j}{\partial a^t_j}\\
            \frac{\partial a^{t+1}_j}{\partial v^t_j} & \frac{\partial a^{t+1}_j}{\partial a^t_j}
            \end{pmatrix}.
            \end{equation}
            The presence of $z^t_j$, and its relation to the Heaviside step function $H(\cdot)$ in the hidden state updates in Equation \ref{eq:alifV} and Equation \ref{eq:alifA} seems problematic for computing these partial derivatives, because the derivative $\frac{\partial z^t_j}{\partial v^t_j}$ is nonexistent.
            This is overcome by replacing it with a simple nonlinear function called a pseudo-derivative.
            Outside of the refractory period of a neuron $j$, this pseudo-derivative has the form
            \begin{equation}
            \psi_j^t = \gamma \max\left(0, 1 - \left|\frac{v_j^t - v_\text{th} - \beta a^t_j}{v_\text{th}}\right|\right),
            \end{equation}
            where $\gamma$ is a dampening constant, which is set to 0 during the neuron's refractory period.
            Like in \citet{esser2016convolutional}, this pseudo-derivative is 1 at time steps where the neuron spikes, and linearly decays to zero in the positive and negative direction.
            The synaptic weight can only change when the pseudo-derivative is nonzero.

            Now, the partial derivatives in the hidden state derivative can be computed by replacing the Heaviside function Equation (in \ref{eq:alifZ}) by the pseudo-derivative $\psi^t_j$:
            \begin{align}
            \frac{\partial v_j^{t+1}}{\partial v_j^t} &= \alpha\\
            \frac{\partial v_j^{t+1}}{\partial a_j^t} &= 0\\
            \frac{\partial a_j^{t+1}}{\partial v_j^t} &= \psi^t_j\\
            \frac{\partial a_j^{t+1}}{\partial a_j^t} &= \rho - \psi^t_j\beta.
            \end{align}
            These partial derivatives can be used to compute the eligibility vector:
            \begin{align}
            \begin{pmatrix}
            \epsilon_{ji, v}^{t+1}\\
            \epsilon_{ji, a}^{t+1}
            \end{pmatrix}
            &=
            \begin{pmatrix}
            \frac{\partial v^{t+1}_j}{\partial v^t_j} & \frac{\partial v^{t+1}_j}{\partial a^t_j}\\
            \frac{\partial a^{t+1}_j}{\partial v^t_j} & \frac{\partial a^{t+1}_j}{\partial a^t_j}
            \end{pmatrix}
            \cdot
            \begin{pmatrix}
            \epsilon_{ji, v}^t\\
            \epsilon_{ji, a}^t
            \end{pmatrix}
            +
            \begin{pmatrix}
            \frac{\partial v^{t+1}_j}{\partial W_{ji}}\\
            \frac{\partial a^{t+1}_j}{\partial W_{ji}}
            \end{pmatrix}\\
            &=
            \begin{pmatrix}
            \alpha & 0\\
            \psi^t_j & \rho-\psi^t_j\beta
            \end{pmatrix}
            \cdot
            \begin{pmatrix}
            \epsilon_{ji, v}^t\\
            \epsilon_{ji, a}^t
            \end{pmatrix}
            +
            \begin{pmatrix}
            z_i^{t-1}\\
            0
            \end{pmatrix}\label{eq:evector_b}\\
            &=
            \begin{pmatrix}
            \alpha \cdot\epsilon_{ji, v}^t + z_i^{t-1}\\
            \psi^t_j\epsilon^t_{ji, v} + \left(\rho-\psi^t_j\beta\right)\epsilon^t_{ji, a}
            \end{pmatrix}.
            \end{align}

            Intuitively, these eligibility vector components can be seen as the contribution of the hidden state component to the increase of the eligibility trace.
            For instance, the activation eligibility component $\epsilon^t_{ji,v}$ of a synapse $i\rightarrow j$ at time step $t$ is, like in the LIF neuron, a low-pass filter of the afferent spikes $z_i$.

            The threshold adaptation eligibility component $\epsilon^t_{ji,a}$ is less intuitive, but acts as a correction factor for the more slowly decaying threshold adaptation.
            Its first term $\psi^t_j\epsilon^t_{ji,v}$ causes it to increase when a neuron has recently spiked and when the activation is already increasing again.
            Therefore, it is higher for synapses that have a higher spike frequency.
            The second term threshold adaptation eligibility component is a decay corrected for the adaptation strength $\beta$.

            This eligibility vector update can be recursively applied.
            For eligibility vectors of synapses that are efferent to input neurons, the input value $x^t_i$ is used in place of $z_i^{t-1}$ in Equation \ref{eq:evector_b}.
            Note that the current time index $t$ is used for input neurons to satisfy the online learning principle defined in the model definition in Equation \ref{eq:model}; neurons receive input from the input at time $t$, and from the spikes of other neurons emitted at time $t-1$.
            Furthermore, the absence of $\epsilon_{ji, a}^t$ in the computation of $\epsilon_{ji, v}^{t+1}$ facilitates online training in emulations in non--von Neumann machines, because $\epsilon_{ji, a}^{t+1}$ can be computed before $\epsilon_{ji, v}^{t+1}$, relieving the need to store a temporary copy of its value.
            In later sections, it is demonstrated that this does not necessarily hold for other neuron models, such as the Izhikevich neuron.

            The eligibility vector needs to be multiplied with the partial derivative of the observable state with respect to the hidden state to obtain the eligibility trace:
            \begin{equation}
            e^t_{ji} = \bm{\epsilon}_{ji}^t \cdot \frac{\partial z^t_j}{\partial\mathbf{h}^t_j}.
            \end{equation}

            Again, the Heaviside function in Equation \ref{eq:alifZ} is replaced by $\psi^t_j$:
            \begin{align}
                \frac{\partial z^t_j}{\partial\mathbf{h}^t_j} &= \begin{pmatrix}
                    \frac{\partial z^t_j}{\partial v^t_j}\\
                    \frac{\partial z^t_j}{\partial a^t_j}
                    \end{pmatrix}\\
                &= \begin{pmatrix}
                    \psi^t_j\\
                    -\beta\psi^t_j
                    \end{pmatrix}.
            \end{align}

            Therefore, the eligibility trace is computed by
            \begin{align}
               e^t_{ji} &= \begin{pmatrix}
            \epsilon_{ji, v}^t\\
            \epsilon_{ji, a}^t
            \end{pmatrix}\cdot \begin{pmatrix}
                    \frac{\partial z^t_j}{\partial v^t_j}\\
                    \frac{\partial z^t_j}{\partial a^t_j}
                    \end{pmatrix}\\
                &= \begin{pmatrix}
            \epsilon_{ji, v}^t\\
            \epsilon_{ji, a}^t
            \end{pmatrix}\cdot \begin{pmatrix}
                    \psi^t_j\\
                    -\beta\psi^t_j
                    \end{pmatrix}\\
                    &= \psi^t_j\left(\epsilon_{ji, v}^t - \beta\epsilon_{ji, a}^t\right).
            \end{align}


            This means that the eligibility trace can be understood as a low-pass filter of the afferent spikes, with a correction for the efferent neuron's threshold adaptation: a neuron with a higher threshold builds up an eligibility trace more slowly than its more sensitive counterparts.
            Figure \ref{fig:alif} illustrates the behavior of the synaptic variables in an ALIF neuron described above.

            \begin{figure}[ht]
                \centering
                \includegraphics[width=\linewidth]{gfx/alif}
                \caption[Single-synapse ALIF simulation]{A single-synapse simulation of the evolution of the full hidden state of the ALIF neuron. The blue lines indicate the postsynaptic neuron $j$, and the orange lines indicate the presynaptic neuron $i$. The injected current $I^t$ increases the voltage $v^t_j$ and is deliberately controlled to produce the spike pattern $z^t_j$ where the postsynaptic neuron spikes after the presynaptic neuron during the first half, and vice versa during the second half of the plot. The learning signal $L^t_j$ is kept at a constant value and is omitted for clarity, such that the relation between the eligibility trace $e^t_{ji}$ and the accumulated weight change $\Delta W^t_{ji}$ can be clearly observed. Note that the synapse weight increases regardless of the order of spikes, indicating an absence of STDP in the standard e-prop ALIF neuron.}
                \label{fig:alif}
            \end{figure}

        \subsubsection{Gradients}\label{sec:eprop_grd}
            Gradient descent is used to apply the weight updates, such that weights are updated by a small fraction $\eta$ in the negative direction of the estimated gradient of the loss function with respect to the model weights:
            \begin{equation}\label{eq:eprop_grd}
                \Delta W = -\eta\widehat{\frac{dE}{dW_{ji}}} \eqdef -\eta\sum_t\frac{\partial E}{\partial z^t_j}e^t_{ji}.
            \end{equation}
            Note that for clarity, this section describes e-prop using stochastic gradient descent.
            In the actual implementations in \citet{bellec2020solution} and this research, the Adam optimization algorithm \citep{kingma2014adam} is used (see Section \ref{sec:adam}).

            \paragraph{Error metric}
            In the TIMIT frame-wise phone classification task, there are $K=61$ output neurons $y^t_k$ where $k \in [1\mathrel{{.}\,{.}}\nobreak K]$.
            These are computed according to
            \begin{equation}\label{eq:bellec_y}
            y^t_k = \kappa y^{t-1}_k + \sum_jW^\text{out}_{kj}z^t_j + b_k,
            \end{equation}
            where $\kappa \in [0, 1]$ is the decay factor for the output neurons, $W^\text{out}_{kj}$ is the weight between neuron $j$ and output neuron $k$, and $b_k$ is the bias value.
            The decay factor $\kappa$ acts as a low-pass filter, smoothing the output values over time and implemented based on the observation that output frame classes typically persist for multiple time steps.

            The softmax function $\sigma(\cdot)$ computes the predicted probability $\pi^t_k$ for class $k$ at time $t$:
            \begin{equation}
            \pi^t_k = \sigma_k\left(y^t_1,\ldots,y^t_K\right) = \frac{\exp\left(y^t_k\right)}{\sum_{k'}\exp\left(y^t_{k'}\right)}.
            \end{equation}
            This predicted probability is compared with the one-hot vector corresponding to the target class label $\pi^{*,t}_k$ at time step $t$ using the cross entropy loss function
            \begin{equation}
            E = -\sum_{t,k}\pi^{*,t}_k\log\pi^t_k,
            \end{equation}
            thereby obtaining the accumulated loss $E$ at time step $t$.

            Since the learning signal $L^t_j$ is defined as the partial derivative of the error $E$ with respect to the observable state $z_j^t$ of a neuron $j$ afferent to an output neuron $k$, we can use
            \begin{equation}\label{eq:learningsignal_after_output}
            L^t_j = \frac{\partial E}{\partial z^t_j} = \sum_kB_{jk}\sum_{t'\geq t}\left(\pi^{t'}_k - \pi^{*,t'}_k\right)\kappa^{t'-t},
            \end{equation}
            where $B_{jk}$ is a feedback weight from neuron $k$ back to output neuron $j$.
            There are multiple strategies for choosing feedback weights.
            \citet{bellec2020solution} noted that a constantly uniform weight matrix yields poor performance, which has been empirically verified in this report.
            However, when the feedback weight matrix is initialized from a zero-centered normal distribution, it can remain either constant, mirror $(W^\text{out})^\top$, or update according to $(\Delta W^\text{out})^\top$.
            These three variants are referred to in \citet{bellec2020solution} as \emph{random}, \emph{symmetric}, and \emph{adaptive} e-prop, respectively.
            In this paper, symmetric e-prop is used (i.e., $B_{jk} \eqdef W^\text{out}_{kj}$) unless explicitly stated otherwise.

            Note that the term $\kappa^{t'-t}$ in Equation \ref{eq:learningsignal_after_output} is a filter that compensates for the decay factor of output neurons.
            Note also that this equation does not allow online learning, because future time steps $t'$ are accessed.
            However, if a low-pass filter with factor $\kappa$ is applied on the eligibility trace, it will cancel out the effects of the future time steps on the learning signal, and the estimated loss gradient can be approximated.
            This low-pass filter of the eligibility trace can be implemented in an online fashion by including it as a hidden synaptic variable $\bar{e}^t_{ji}$.
            Recall that the estimated loss gradient $\widehat{\frac{dE}{dW_{ji}}}$ is approximated by $\sum_t \frac{\partial E}{\partial z^t_j}e^t_{ji}$.
            Therefore, after inserting Equation \ref{eq:learningsignal_after_output} in Equation \ref{eq:eprop_grd}, the weight update is computed by
            \begin{align}
            \Delta W_{ji} &= -\eta\sum_{t'}\frac{\partial E}{\partial z^{t'}_j}e^{t'}_{ji}\\
            &= -\eta\sum_{t'}\sum_kB_{jk}\sum_{t'\geq t}\left(\pi^{t'}_k - \pi^{*,t}_k\right)\kappa^{t'-t}e^{t'}_{ji}\\
            &= -\eta\sum_{k, t'}B_{jk}\sum_{t'\geq t}\left(\pi^{t'}_k - \pi^{*,t}_k\right)\kappa^{t'-t}e^{t'}_{ji}\\
            &= -\eta\sum_t\underbrace{\sum_kB_{jk}\left(\pi^{t}_k - \pi^{*,t}_k\right)}_{=L^t_j}\underbrace{\sum_{t'\leq t}\kappa^{t'-t}e^{t'}_{ji}}_{\eqdef \bar{e}^t_{ji}}\label{dwlast},
            \end{align}
            where $W_{ji}$ is an input or recurrent weight.
            By implementing $\bar{e}_{ji}$ as a low-pass filter (with factor $\kappa$) of the eligibility trace, the weight update in Equation \ref{dwlast} is implemented as a local and online learning algorithm.

            The training algorithm for the output weights $W^\text{out}$ and bias $b$ can be directly derived from gradient descent:
            \begin{equation}
            \Delta W^\text{out}_{kj} = -\eta \sum_t\left(\pi^t_k - \pi^{*,t}_k\right)\sum_{t'\leq t}\kappa^{t'-t}z^t_j
            \end{equation}
            and
            \begin{equation}
            \Delta b_k = -\eta \sum_t\left(\pi^t_k - \pi^{*,t}_k\right).
            \end{equation}


