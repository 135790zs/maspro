\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, yfonts}
\usepackage{algpseudocode}
\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}


\newcommand\eqdef{\ \mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\scriptsize\sffamily def}}}{=}}\ }

\title{E-prop maths}
\author{Werner van der Veen}
\date{\today}

\begin{document}

\maketitle

% \tableofcontents
\section{Proof: BPTT to E-prop}
The main equation to be proved:
\begin{equation}
\frac{dE}{dW_{ji}} = 
\sum_t\frac{dE}{dz_j^t}\cdot\left[\frac{dz_j^t}{dW_{ji}}\right]_\text{local}
\end{equation}

We start with the classical factorization of the loss gradients in an unrolled RNN:
\begin{equation}\label{eq:clafac}
\frac{dE}{dW_{ji}} = \frac{dE}{d\mathbf{h}_j^{t'}}\cdot\frac{\partial \mathbf{h}_j^{t'}}{\partial W_{ji}}
\end{equation}
The summation indicates that weights are shared in an unrolled RNN.

We now decompose the first term into a series of learning signals $L_j^t = \frac{dE}{dz_j^t}$ and local factors $\frac{\partial\mathbf{h}_j^{t-t'}}{\partial\mathbf{h}_j^t}$ for $t$ since the event horizon $t'$:

\begin{equation}\label{eq:rec}
\frac{dE}{d\mathbf{h}_j^{t'}} = \underbrace{\frac{dE}{dz_j^{t'}}}_{L^{t'}_j} \frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \frac{dE}{d\mathbf{h}_j^{t'+1}}\frac{\partial\mathbf{h}_j^{t'+1}}{\partial\mathbf{h}_j^{t'}}
\end{equation}
Note that this equation is recursive.
If we substitute the equation (\ref{eq:rec}) into the classical factorization (\ref{eq:clafac}), we get:
\begin{equation}
\frac{dE}{dW_{ji}} = \sum_{t'}\left(L_j^{t'}\frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \frac{dE}{d\mathbf{h}_j^{t'+1}}\frac{\partial\mathbf{h}_j^{'t+1}}{\partial\mathbf{h}_j^{'t}}\right)\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}
\end{equation}
\begin{equation}
= \sum_{t'}\left(L_j^{t'}\frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \left( L^{t'+1}_j \frac{\partial z_j^{t'+1}}{\partial\mathbf{h}_j^{t'+1}} + (\cdots)\frac{\partial\mathbf{h}_j^{t'+2}}{\partial\mathbf{h}_j^{t'+1}}  \right) \frac{\partial\mathbf{h}_j^{'t+1}}{\partial\mathbf{h}_j^{'t}}\right)\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}
\end{equation}

We write the term in parentheses into a second term indexed by $t$:
\begin{equation}
\frac{dE}{dW_{ji}} = \sum_{t'}\sum_{t\geq t'}L^t_j\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{'t}}{\partial W_{ji}}
\end{equation}

We then exchange the summation indices to pull out the learning signal $L_j^t$. This expresses the loss as a sum of learning signals multiplied by something we define as the eligibility trace. This eligibility trace consists of $\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}$ and the eligibility vector $\mathbf{\epsilon}_{ji}^t$:
\begin{equation}
\frac{dE}{dW_{ji}} = \sum_tL^t_j\underbrace{\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\underbrace{\sum_{t\geq t'}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{'t}}{\partial W_{ji}}}_{\mathbf{\epsilon}_{ji}^t}}_{e^t_{ji}}
\end{equation}
This is the main e-prop equation.

\section{Single-layer e-prop in pseudocode (LIF)}
In LIF, $\{\mathbf{h}^t_j, \mathbf{\epsilon}^t_{ji}\} \subset \mathbb{R}$.

\begin{algorithmic}
\For{$t$ in $T$}
	\State $z_j^t \gets \begin{cases}0, & \text{if}\ t - t_{z_j} < \delta t_\text{ref}.\\H(v_j^t - v_\text{th}), & \text{otherwise}.\end{cases}$
	\vspace{10pt}
	\State $I^t_j \gets \sum_i W_{ji}z_i^t + \sum_u W_{ju}u(t) $
	\vspace{10pt}
	\State $v_j^{t+1} \gets \alpha v_j^t + I_j^t - z_j^t\alpha v_j^t - z^{t-\delta t_\text{ref}}_j\alpha v_j^t$
	\vspace{10pt}
	\State $\epsilon^{t+1}_{ji} = \alpha(1-z_j-z_j^{t-\delta t_\text{ref}})\epsilon^t_{ji} + z^t_i$
	\vspace{10pt}
	\State $h_j^{t+1} \gets \begin{cases}-\gamma, & \text{if}\ t - t_{z_j} < \delta t_\text{ref}.\\\gamma\max\left(0, 1-\left|\frac{v_j^{t+1}-v_\text{th}}{v_\text{th}}\right|\right), & \text{otherwise}.\end{cases}$
	\vspace{10pt}
	\State $e^{t+1}_{ji} \gets h^{t+1}_j\epsilon^{t+1}_{ji}$
	\vspace{10pt}
	\State $y^t_k = \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj} z^t_j + b^\text{out}_k$
	\vspace{10pt}
	\State $W \gets W - \eta \sum_t\left(\sum_k B_{jk}\left(y_k^t - y_k^{*,t}\right)\right)e^t_{ji}$
	
\EndFor
\end{algorithmic}

\section{ALIF steps}
\begin{align}
I^t_j &= \sum_{t\neq j} W^\text{rec}_{ji} z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} \\
A^t_j &= v_\text{th}+\beta a^t_j\\
z^t_j &= H\left(v_j^t-A_j^t\right)\\
\psi^t_j &= \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-A^t_j}{v_\text{th}}\right|\right)\\
y^t_k &= \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj}z^t_j+b^\text{out}_k\\
e^t_{ji} &= \psi^t_j\left(\epsilon^t_{ji,v} - \beta\epsilon^t_{ji,a}\right)\\
\bar{e}^t_{ji} &= \kappa\bar{e}^{t-1}_{ji} + e^t_{ji}\\
v^{t+1}_j &= \alpha v_j^t + I_j^t-z_jv_\text{th}\\
a^{t+1}_j &= \rho a_j^t + z_j^t\\
\epsilon_{ji,v}^{t+1} &= \alpha\epsilon_{ji,v}^t + z_i^t\\
\epsilon_{ji,a}^{t+1} &= \psi^t_j\epsilon^t_{ji,v} + \left(\rho-\psi^t_j\beta\right)\epsilon^t_{ji,a}\\
\Delta W^\text{rec}_{ji} &= -\eta\sum_t\left(\sum_kB_{jk}\left(y_k^t-y_k^{*,t}\right)\right)\bar{e}^t_{ji}\\
\end{align}

Given at time $t$, with given observable state $z_i^t$ (simplified):
\begin{align}
v^{t+1}_j &= \alpha v_j^t + \sum_{t\neq j} W^{\text{rec}, t}_{ji} z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} - H\left(v_j^t-v_\text{th}-\beta a^t_j\right)v_\text{th}\\
a^{t+1}_j &= \rho a_j^t + H\left(v_j^t-v_\text{th}-\beta a^t_j\right)\\
\epsilon_{ji,v}^{t+1} &= \alpha\epsilon_{ji,v}^t + z_i^t\\
\epsilon_{ji,a}^{t+1} &= \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}-\beta a^t_j}{v_\text{th}}\right|\right)\epsilon^t_{ji,v} \\ &\quad \ + \left(\rho-\frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}-\beta a^t_j}{v_\text{th}}\right|\right)\beta\right)\epsilon^t_{ji,a}\\
\bar{e}^t_{ji} &= \kappa\bar{e}^{t-1}_{ji} + \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}-\beta a^t_j}{v_\text{th}}\right|\right)\left(\epsilon^t_{ji,v} - \beta\epsilon^t_{ji,a}\right)\\
y^t_k &= \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj}z^t_j+b^\text{out}_k\\
W^{\text{rec}, t+1}_{ji} &= W^{\text{rec}, t}_{ji} -\eta\sum_t\left(\sum_kB_{jk}\left(y_k^t-y_k^{*,t}\right)\right)\bar{e}^t_{ji}\\
\end{align}



Effects of $\mathbf{h}$ on $W$:
\begin{align}
\frac{\partial v_j^t}{\partial W_{ji}} &= \epsilon_{ji,v}^{t-1} \\
\frac{\partial u_j^t}{\partial W_{ji}} &= 0
\end{align}












\end{document}
