\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, yfonts}
\usepackage{algpseudocode}
\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}


\newcommand\eqdef{\ \mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\scriptsize\sffamily def}}}{=}}\ }

\title{E-prop maths}
\author{Werner van der Veen}
\date{\today}

\begin{document}

\maketitle

% \tableofcontents
\section{Proof: BPTT to E-prop}
The main equation to be proved:
\begin{equation}
\frac{dE}{dW_{ji}} = 
\sum_t\frac{dE}{dz_j^t}\cdot\left[\frac{dz_j^t}{dW_{ji}}\right]_\text{local}
\end{equation}

We start with the classical factorization of the loss gradients in an unrolled RNN:
\begin{equation}\label{eq:clafac}
\frac{dE}{dW_{ji}} = \frac{dE}{d\mathbf{h}_j^{t'}}\cdot\frac{\partial \mathbf{h}_j^{t'}}{\partial W_{ji}}
\end{equation}
The summation indicates that weights are shared in an unrolled RNN.

We now decompose the first term into a series of learning signals $L_j^t = \frac{dE}{dz_j^t}$ and local factors $\frac{\partial\mathbf{h}_j^{t-t'}}{\partial\mathbf{h}_j^t}$ for $t$ since the event horizon $t'$:

\begin{equation}\label{eq:rec}
\frac{dE}{d\mathbf{h}_j^{t'}} = \underbrace{\frac{dE}{dz_j^{t'}}}_{L^{t'}_j} \frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \frac{dE}{d\mathbf{h}_j^{t'+1}}\frac{\partial\mathbf{h}_j^{t'+1}}{\partial\mathbf{h}_j^{t'}}
\end{equation}
Note that this equation is recursive.
If we substitute the equation (\ref{eq:rec}) into the classical factorization (\ref{eq:clafac}), we get:
\begin{equation}
\frac{dE}{dW_{ji}} = \sum_{t'}\left(L_j^{t'}\frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \frac{dE}{d\mathbf{h}_j^{t'+1}}\frac{\partial\mathbf{h}_j^{'t+1}}{\partial\mathbf{h}_j^{'t}}\right)\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}
\end{equation}
\begin{equation}
= \sum_{t'}\left(L_j^{t'}\frac{\partial z_j^{t'}}{\partial\mathbf{h}_j^{t'}} + \left( L^{t'+1}_j \frac{\partial z_j^{t'+1}}{\partial\mathbf{h}_j^{t'+1}} + (\cdots)\frac{\partial\mathbf{h}_j^{t'+2}}{\partial\mathbf{h}_j^{t'+1}}  \right) \frac{\partial\mathbf{h}_j^{'t+1}}{\partial\mathbf{h}_j^{'t}}\right)\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}
\end{equation}

We write the term in parentheses into a second term indexed by $t$:
\begin{equation}
\frac{dE}{dW_{ji}} = \sum_{t'}\sum_{t\geq t'}L^t_j\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{'t}}{\partial W_{ji}}
\end{equation}

We then exchange the summation indices to pull out the learning signal $L_j^t$. This expresses the loss as a sum of learning signals multiplied by something we define as the eligibility trace. This eligibility trace consists of $\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}$ and the eligibility vector $\mathbf{\epsilon}_{ji}^t$:
\begin{equation}
\frac{dE}{dW_{ji}} = \sum_tL^t_j\underbrace{\frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\underbrace{\sum_{t\geq t'}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{'t}}{\partial W_{ji}}}_{\mathbf{\epsilon}_{ji}^t}}_{e^t_{ji}}
\end{equation}
This is the main e-prop equation.

\section{Single-layer e-prop in pseudocode (LIF)}
In LIF, $\{\mathbf{h}^t_j, \mathbf{\epsilon}^t_{ji}\} \subset \mathbb{R}$.

\begin{algorithmic}
\For{$t$ in $T$}
	\State $z_j^t \gets \begin{cases}0, & \text{if}\ t - t_{z_j} < \delta t_\text{ref}.\\H(v_j^t - v_\text{th}), & \text{otherwise}.\end{cases}$
	\vspace{10pt}
	\State $I^t_j \gets \sum_i W_{ji}z_i^t + \sum_u W_{ju}u(t) $
	\vspace{10pt}
	\State $v_j^{t+1} \gets \alpha v_j^t + I_j^t - z_j^t\alpha v_j^t - z^{t-\delta t_\text{ref}}_j\alpha v_j^t$
	\vspace{10pt}
	\State $\epsilon^{t+1}_{ji} = \alpha(1-z_j-z_j^{t-\delta t_\text{ref}})\epsilon^t_{ji} + z^t_i$
	\vspace{10pt}
	\State $h_j^{t+1} \gets \begin{cases}-\gamma, & \text{if}\ t - t_{z_j} < \delta t_\text{ref}.\\\gamma\max\left(0, 1-\left|\frac{v_j^{t+1}-v_\text{th}}{v_\text{th}}\right|\right), & \text{otherwise}.\end{cases}$
	\vspace{10pt}
	\State $e^{t+1}_{ji} \gets h^{t+1}_j\epsilon^{t+1}_{ji}$
	\vspace{10pt}
	\State $y^t_k = \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj} z^t_j + b^\text{out}_k$
	\vspace{10pt}
	\State $W \gets W - \eta \sum_t\left(\sum_k B_{jk}\left(y_k^t - y_k^{*,t}\right)\right)e^t_{ji}$
	
\EndFor
\end{algorithmic}
\section{LIF steps}
\begin{align}
I^t_j &= \sum_{i\neq j} W^\text{rec}_{ji} z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} \\
z^t_j &= H\left(v_j^t-v_\text{th}\right)\\
\psi^t_j &= \frac{1}{v_\text{th}}\gamma_{pd}\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}}{v_\text{th}}\right|\right)\\
y^t_k &= \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj}z^t_j+b^\text{out}_k\\
e^{t+1}_{ji} &= \psi^{t+1}_j\bar{z}^t_i\\
\bar{e}^t_{ji} &= \kappa\bar{e}^{t-1}_{ji} + e^t_{ji}\\
v^{t+1}_j &= \alpha v_j^t + I_j^t-z_jv_\text{th}\\
\Delta W^\text{rec}_{ji} &= -\eta\sum_t\left(\sum_kB_{jk}\left(y_k^t-y_k^{*,t}\right)\right)\bar{e}^t_{ji}\\
\end{align}
\section{ALIF steps}
\begin{align}
I^t_j &= \sum_{i\neq j} W^\text{rec}_{ji} z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} \\
A^t_j &= v_\text{th}+\beta a^t_j\\
z^t_j &= H\left(v_j^t-A_j^t\right)\\
\psi^t_j &= \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-A^t_j}{v_\text{th}}\right|\right)\\
y^t_k &= \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj}z^t_j+b^\text{out}_k\\
e^t_{ji} &= \psi^t_j\left(\epsilon^t_{ji,v} - \beta\epsilon^t_{ji,a}\right)\\
\bar{e}^t_{ji} &= \kappa\bar{e}^{t-1}_{ji} + e^t_{ji}\\
v^{t+1}_j &= \alpha v_j^t + I_j^t-z_jv_\text{th}\\
a^{t+1}_j &= \rho a_j^t + z_j^t\\
\epsilon_{ji,v}^{t+1} &= \alpha\epsilon_{ji,v}^t + z_i^t\\
\epsilon_{ji,a}^{t+1} &= \psi^t_j\epsilon^t_{ji,v} + \left(\rho-\psi^t_j\beta\right)\epsilon^t_{ji,a}\\
\pi_k^t &= \frac{\exp\left(y_k^t\right)}{\sum_{k'}\exp\left( y^t_{k'}\right)}\\
E &= -\sum_{t,k}\pi_k^{*,t}\log\pi_k^t\\
\Delta W^\text{rec}_{ji} &= -\eta\sum_t\left(\sum_kB_{jk}\left(\pi_k^t-\pi_k^{*,t}\right)\right)\bar{e}^t_{ji}\\
\end{align}

\section{ALIF steps (multilayer)}
Layer $r \in \left\{1, \ldots, R\right\}$, time $t \in \left\{1, \ldots, T\right\}$:
\begin{align}
z^t_{r} &= H\left(v^t-A^t\right)\\
z_\text{prev}^t &= \begin{cases}z^t_{r-1},&\text{if }r > 1\\x^t,&\text{otherwise}\end{cases}\\
{z_\text{in}}^t_{r} &= \text{concat}(z_\text{prev}^t, z^t_{r})\\
I^t_{r} &= W^\text{rec}_r \cdot {z_\text{in}}^t_{r} \\
\psi^t_r &= \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v^t_r-A^t_r}{v_\text{th}}\right|\right)\\
\epsilon_{v, r}^{t+1} &= \alpha\epsilon_{v, r}^t + {z_\text{in}}_r^t\\
\epsilon_{a, r}^{t+1} &= \psi^t_r\epsilon_{v, r}^t + \left(\rho - \psi^t_r\beta\right)\epsilon_{a, r}^t\\
v^{t+1}_r &= \alpha v_r^t + I_r^t-z_rv_\text{th}\\
a^{t+1}_r &= \rho a_r^t + z_r^t\\
e^t_r &= \psi^t\left(\epsilon^t_{v, r} - \beta\epsilon^t_{a, r}\right)\\
\bar{e}^t_r &= \kappa\bar{e}^{t-1}_r + e^t_r\\
\bar{z}^t_r &= \kappa\bar{z}^{t-1}_r + z^t_r\\
y^t &= \kappa y^{t-1} + \sum_j W^\text{out}z^t_{R}+b^\text{out}\\
\pi^t &= \frac{\exp\left(y^t\right)}{\sum_{k'}\exp\left( y^t_{k'}\right)}\\
\Delta W^\text{rec}_r &= -\eta\sum_tB \cdot\left(\pi^t-\pi^{*,t}\right)\bar{e}^t_r\\
\Delta W^\text{out} &= -\eta\sum_t\left(\pi^t-\pi^{*,t}\right)\bar{z}^t_R\\
\Delta b^\text{out} &= -\eta\sum_t\left(\pi^t-\pi^{*,t}\right)\\
E &= -\sum_{t, k}\pi_k^{*,t}\log\pi_k^t
\end{align}

Given at time $t$, with given observable state $z_i^t$ (simplified):
\begin{align}
v^{t+1}_j &= \alpha v_j^t + \sum_{t\neq j} W^{\text{rec}, t}_{ji} z_i^t + \sum_i W^\text{in}_{ji}x_i^{t+1} - H\left(v_j^t-v_\text{th}-\beta a^t_j\right)v_\text{th}\\
a^{t+1}_j &= \rho a_j^t + H\left(v_j^t-v_\text{th}-\beta a^t_j\right)\\
\epsilon_{ji,v}^{t+1} &= \alpha\epsilon_{ji,v}^t + z_i^t\\
\epsilon_{ji,a}^{t+1} &= \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}-\beta a^t_j}{v_\text{th}}\right|\right)\epsilon^t_{ji,v} \\ &\quad \ + \left(\rho-\frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}-\beta a^t_j}{v_\text{th}}\right|\right)\beta\right)\epsilon^t_{ji,a}\\
\bar{e}^t_{ji} &= \kappa\bar{e}^{t-1}_{ji} + \frac{1}{v_\text{th}}0.3\max\left(0, 1-\left|\frac{v_j^t-v_\text{th}-\beta a^t_j}{v_\text{th}}\right|\right)\left(\epsilon^t_{ji,v} - \beta\epsilon^t_{ji,a}\right)\\
y^t_k &= \kappa y^{t-1}_k + \sum_j W^\text{out}_{kj}z^t_j+b^\text{out}_k\\
W^{\text{rec}, t+1}_{ji} &= W^{\text{rec}, t}_{ji} -\eta\sum_t\left(\sum_kB_{jk}\left(y_k^t-y_k^{*,t}\right)\right)\bar{e}^t_{ji}\\
\end{align}



Effects of $\mathbf{h}$ on $W$:
\begin{align}
\frac{\partial v_j^t}{\partial W_{ji}} &= \epsilon_{ji,v}^{t-1} \\
\frac{\partial u_j^t}{\partial W_{ji}} &= 0
\end{align}












\newpage
\section{Audio preprocessing}
Goal: turn a .wav signal consisting of N phones into N chunks.

Frame length = 400 samples.

Frame step = 160 samples.

For every frame, 13 MFCC coefficients are extracted.

We take the DFT:
\begin{equation}
S_i(k) = \sum_{n=1}^N s_i(n)h(n)e^{-j2\pi kn/N}\qquad1\leq k\leq K
\end{equation}
where $h(n)$ is an $N$ sample long analysis window (e.g. hamming window), and $K$ is the length of the DFT (in our case, $K=512$).

We then take the periodogram estimate.
\begin{equation}
P_i(k) = \frac{1}{N}\left|S_i(k)\right|^2
\end{equation}
where we then keep only the first 257 coefficients.

We have a filterbank of 26 triangular filters of length 257. Each filter is mostly zero, except its specific region.

To compute the Mel filterbank, we choose lower and upper frequencies of 0Hz and 8000Hz and we convert them to Mels using
\begin{equation}
M(f) = 1125\ln\left(1 + f/700\right)
\end{equation}
We obtain 0 Mels and 2834.99 Mels.

To obtain our 26 filterbanks, we need 26 points spaced linearly between 0 and 2834.99:
\begin{verbatim}
[   0.        ,  104.99962963,  209.99925926,  314.99888889,
  419.99851852,  524.99814815,  629.99777778,  734.99740741,
  839.99703704,  944.99666667, 1049.9962963 , 1154.99592593,
 1259.99555556, 1364.99518519, 1469.99481481, 1574.99444444,
 1679.99407407, 1784.9937037 , 1889.99333333, 1994.99296296,
 2099.99259259, 2204.99222222, 2309.99185185, 2414.99148148,
 2519.99111111, 2624.99074074, 2729.99037037, 2834.99      ]
\end{verbatim}

We then convert these back to Hertz using
\begin{equation}
M^{-1}(m) = 700\left(\exp\left(m/1125\right)-1\right)
\end{equation}
and obtain
\begin{verbatim}
[   0.        ,   68.47907878,  143.65727789,  226.18995388,
  316.79657505,  416.26699328,  525.46832953,  645.35253278,
  776.96467861,  921.45207944, 1080.07428613, 1254.21406795,
 1445.38946668, 1655.26702996, 1885.67633923, 2138.62595885,
 2416.32094551, 2721.18207055, 3055.86692273, 3423.29307542,
 3826.6635202 , 4269.49458849, 4755.6466048 , 5289.35753858,
 5875.27994818, 6518.52153898, 7224.68968918, 7999.94033136]
\end{verbatim}

We round these frequencies to the nearest FFT bin, using
\begin{equation}
f(h) = \lfloor(512+1)h/16000\rfloor
\end{equation}
resulting in the sequence
\begin{verbatim}
[  0.,   2.,   4.,   7.,  10.,  13.,  16.,  20.,  24.,  29.,  34.,
  40.,  46.,  53.,  60.,  68.,  77.,  87.,  97., 109., 122., 136.,
 152., 169., 188., 209., 231., 256.]
\end{verbatim}

We can use this to finally create our Mel-spaced filterbank:
\begin{equation}
H_m(k) = \begin{cases}
0 & k<f(m-1)\\
\frac{k-f(m-1)}{f(m)-f(m-1)} & f(m-1)\leq k \leq f(m) \\
\frac{f(m+1) - k}{f(m+1)-f(m)} & f(m) \leq k \leq f(m+1)\\
0 & k > f(m+1)
\end{cases}
\end{equation}


To calculate filterbank energies, we multiply each filterbank with the power spectrum, then add up the coefficients. We then obtain 26 values that indicate how much energy was in each filterbank.

We then take the log of each of these 26 energies.

Finally, we take the DCT of the 26 log filterbank energies to obtain 26 cepstral coefficients. We then retain only the first 13 of the coefficients for each frame.

We also compute the first and second derivatives (delta and delta-delta coefficients, resp.) of the 13 coefficients using
\begin{equation}
d_t = \frac{\sum^N_{n=1}n\left(c_{t+n}-c_{t-n}\right)}{2\sum_{n=1}^Nn^2}
\end{equation}
where $d_t$ is a delta coefficient from frame $t$ in terms of the static coefficients $c_{t+n}$ to $c_{t-n}$, using $N=2$:
\begin{equation}
d_t = \frac{c_{t+1}-c_{t-1}+2c_{t+2}-2c_{t-2}}{10}
\end{equation}

Second derivatives are calculated from the deltas, not the static coefficients.

We finally concatenate the zeroth, first, and second derivatives and acquire a vector of length 39.

\section{Bellec's TIMIT implementatation}
Key points:
\begin{itemize}
\item $N\times39$ audio frames and \\
$N\times61$ class labels.
\item Bi-directional LSNN.
\item 300 LIF, 100 ALIF, $\theta_m = 20$ ms, $\theta_a = 200$ ms, $\beta = 0.184$, $v_\text{th} = 1.6$, $\theta_\text{out} = 3$ ms, refractory period of 2 ms.
\item Train, validation, test size = 3696, 400, 192 sequences. Input $\mathbf{x}^t$ is [0,1]--normalized concatenation of zeroth to second derivatives of MFCCs.
\item Each frame in the sequence is fed for 5 consecutive 1 ms steps.
\item Networks trained for 80 epochs with early stopping on lowest validation error.
\item Adam optimizer (with defaults except $\epsilon_\text{Adam} = 10^{-5}$.
\item Batch size 32, $\gamma = 10^{-2}$. Adaptive $B_{jk}$ initialized as Gaussian with $\mu = 0$ and $\sigma = 1/n$ with $n$ being neurons in LSNN.
\item L2 regularization with additional term $10^{-5} \cdot \|W\|^2$.
\item In adaptive $B$, L2 weight decay on output and broadcast neurons according to [2] using $c_\text{decay} = 10^{-2}$.
\item Fire rate regularization with $c_\text{reg} = 50$. 
\end{itemize}




\end{document}
