\documentclass[t]{beamer}

\setlength {\marginparwidth }{2cm} 
\setlength {\parskip}{0cm} 
\usepackage{todonotes}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{apacite} 	% Use APA Citation
\presetkeys{todonotes}{inline}{}
\beamertemplatenavigationsymbolsempty
\usefonttheme[onlymath]{serif}

\usepackage{algpseudocode}
\usepackage{enumitem,amssymb, yfonts, bm}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}

% \usetheme{AnnArbor}
% \usetheme{Antibes}
% \usetheme{Bergen}
% \usetheme{Berkeley}https://www.sharelatex.com/project/5b12e1a4f84b363f6f336dab
% \usetheme{Berlin}
% \usetheme{Boadilla}
% \usetheme{boxes}
\usetheme{CambridgeUS}
% \usetheme{Copenhagen}
%\usetheme{Darmstadt}
% \usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
% \usetheme{Hannover}
% \usetheme{Ilmenau}
% \usetheme{JuanLesPins}
\setlength{\parskip}{10pt}

% \newcommand*\vc[1]%
% {\begin{pmatrix}#1\end{pmatrix}}

\newcommand*\vc[1]%
{\left(\begin{array}{cccc}#1\end{array}\right)}


\newcommand\eqdef{\ \mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\scriptsize\rmfamily def}}}{=}}\ }

\title[Eligibility propagation]{Improving eligibility propagation using Izhikevich neurons in a multilayer RSNN.\\\vspace{10pt}
\large{Presentation 5: Evaluation meeting 1}}

\author[Werner]{Werner~van~der~Veen\\\footnotesize\texttt({w.k.van.der.veen.2@student.rug.nl})}\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

%======================================

%\begin{frame}
%    \tableofcontents
%\end{frame}

\small
\section{Project overview}
\begin{frame}{A quick project overview}
	Task: using eligibility propagation to classify phonemes.
	
	Challenges:
	\begin{itemize}[label=--]
		\item Desired bio-plasubility requires online and local learning rules.
		\item Spiking and reccurent network: no good learning algorithm yet.
	\end{itemize}
	
	My own contribution: investigate the benefits of
	\begin{itemize}[label=--]
		\item stacking multiple recurrent layers;
		\item using Izhikevich neurons;
		\item various regularization methods.
	\end{itemize}
		
\end{frame}

\section{Background}
\begin{frame}{Original network architecture}
	\begin{figure}[!ht]
		\input{bellecdiagram.tex}
	\end{figure}
\end{frame}



\begin{frame}{The e-prop learning algorithm -- Main equation}
	
	\begin{align*}
		\frac{dE}{dW_{ji}} 
		&= \sum_tL_j^t \cdot e_{ji}^t
	\end{align*}
	
	\begin{figure}[!ht]
		\includegraphics[width=0.8\linewidth]{eligibility.pdf}
	\end{figure}
	
\end{frame}

\subsection{Key equations}
\begin{frame}{The e-prop learning algorithm -- basic equations ($L^t_j$)}
	
	\begin{align*}
		\frac{dE}{dW_{ji}} 
		&= \sum_tL_j^t \cdot e_{ji}^t
	\end{align*}
	
	\begin{align*}
L^t_j &\eqdef \frac{\partial E}{\partial z^t_j}\\ &= \sum_kB_{jk}\left(\pi^t_k-\pi_k^{*,t}\right)
\end{align*}
	(proof omitted for brevity)
	
\end{frame}

\begin{frame}{The e-prop learning algorithm -- basic equations ($e^t_{ji}$)}

	\begin{align*}
e^t_{ji} &\eqdef \frac{\partial z_j^t}{\partial\mathbf{h}_j^t}\underbrace{\sum_{t'\leq t}\frac{\partial\mathbf{h}^t_j}{\partial\mathbf{h}_j^{t-1}} \cdots \frac{\partial\mathbf{h}_j^{t'+1}}{\partial\mathbf{h}_j^{t'}}\cdot\frac{\partial\mathbf{h}_j^{t'}}{\partial W_{ji}}}_{\eqdef \bm{\varepsilon}_{ji}^t}\\
&= \psi_j^t\bar{z}^{t-1}_i\qquad\text{(for LIF neurons in Bellec et al.)}
\end{align*}
	
	
\end{frame}


\subsection{Bellec et al}

\begin{frame}{The e-prop learning algorithm -- Original LIF}
	
	\begin{figure}[!ht]
		\includegraphics[width=0.8\linewidth]{demo_bellec_lif.pdf}
	\end{figure}
	
\end{frame}

\begin{frame}{Previous results}
\tiny{
Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., \& Maass, W. (2020). A solution to the learning dilemma for recurrent networks of spiking neurons.}
	
\begin{figure}[!ht]
		\includegraphics[width=0.8\linewidth]{timitperformance.png}
	\end{figure}
	
\end{frame}

\subsection{Traub et al}
\begin{frame}{The e-prop learning algorithm -- STDP-corrected LIF}
\tiny{
Traub, M., Butz, M. V., Baayen, R. H., \& Otte, S. (2020, September). Learning Precise Spike Timings with Eligibility Traces. In International Conference on Artificial Neural Networks (pp. 659-669). Springer, Cham.}

	\begin{figure}[!ht]
		\includegraphics[width=0.78\linewidth]{demo_traub_lif.pdf}
	\end{figure}
	
\end{frame}

\section{Proposal}
\begin{frame}{My proposed contribution}
\begin{itemize}[label=--]
\item Implement the algorithm in a \emph{multi-layer} SRNN.
	\begin{figure}[!ht]
		\includegraphics[clip, trim=0cm 25cm 0cm 0cm, width=0.667\linewidth]{BellecDiagramML.pdf}  %lbrt
	\end{figure}
\item Evaluate model designs such as neuron type (e.g. Izhikevich); synaptic delay; and regularization such as metaplasticity and synaptic scaling.
\end{itemize}
	
\end{frame}

\section{Current state}
\begin{frame}{Work done so far}
	\begin{todolist}

    \item[\done] Implement Bellec's model with
    \begin{todolist}
    	\item[\done] 1-layer e-prop RSNN with ALIF neurons;
    	\item[\done] TIMIT conversion to 13 MFCCs and their first and second derivatives;
    	\item[\done] $L^2$ and firing rate regularization;
    	\item[\done] Adam optimizer;
    \item[\done] Bidirectional network.
    \end{todolist}
    \item Obtain Bellec's performance;
    \item Analyze effects of
    \begin{todolist}
    	\item[\done] N-layered network;
    \item Izhikevich neurons;
    	\item Metaplasticity;
    	\item[\done] Synaptic scaling;
    	\item[\done] Traub's fix;
    	\item Synaptic delays.
    \end{todolist}
  \end{todolist}
  
\begin{tikzpicture}[remember picture,overlay]
    \node[xshift=-2cm,yshift=-6.5cm] at (current page.north east) {\includegraphics[clip, trim=0cm 21cm 0cm 0cm, width=0.8\linewidth]{BellecDiagramBi.pdf}};
\end{tikzpicture}
\end{frame}


\begin{frame}{Work since last meeting}
	\begin{todolist}

    \item[\done] Implemented bidirectional network;
    \item[\done] Exponentially improved space complexity by optionally not tracking synapse variables over time or epochs;
    \item[\done] Implemented Bayesian hyperparameter optimization using Gaussian processes;
    \item Worked on synaptic delay but not yet finished.
  \end{todolist}
  
\end{frame}


\begin{frame}{Current observations}

\begin{itemize}[label=--]
\item \[\lim_{e\to\infty} W^e_{\textrm{out}} = \mathbf{0}\]
\[\lim_{e\to\infty} b^e_{\textrm{out}} = \mathbf{0}\]
\item Spike frequency normal but heavily affected by weight initialization. Currently finding good rule of thumb for good scaling per layer. Weights in uniform range of $[0, \frac{C}{N}]$ work well, where $N$ is number of afferent connections.
\end{itemize}

\end{frame}

\begin{frame}{Typical results (Undesired pulsation)}

	\begin{figure}[!ht]
		\includegraphics[clip, trim=0cm 9.28cm 0cm 0cm, width=1\linewidth]{latest_metric.pdf}  %lbrt
	\end{figure}

\end{frame}

\begin{frame}{Typical results (Bellec reproduction attempt)}

	\begin{figure}[!ht]
		\includegraphics[clip, trim=0cm 0cm 0cm 4.2cm, width=0.33\linewidth]{state_195.pdf}  %lbrt
	\end{figure}

\end{frame}

\begin{frame}{Typical results (latest Bellec run)}
	\begin{figure}[!ht]
		Epoch 0\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_0.pdf}\\  %lbrt
		Epoch 25\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_25.pdf}\\  %lbrt
		Epoch 50\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_50.pdf}\\  %lbrt
		Epoch 75\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_75.pdf}\\  %lbrt
		Epoch 100\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_100.pdf}\\  %lbrt
		Epoch 150\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_150.pdf}\\  %lbrt
		Epoch 195\includegraphics[clip, trim=0cm 5.8cm 0cm 28.9cm, width=.6\linewidth]{state_195.pdf}  %lbrt
	\end{figure}

\end{frame}



\begin{frame}{Current configuration toolbox}
	\renewcommand{\baselinestretch}{0.7}
	\footnotesize{
	\begin{itemize}[label=--]

    \item E-prop type (random, symmetric, adaptive) (for updating $B_j^t$)
    \item Optimizer (Adam, SGD)
    \item Traub fix
    \item Fraction ALIF
    \item Uni- or bidirectional
    \item Target delay
    \item Various e-prop model constants
    \item Initial weight scaling
    \item Weight decay
    \item $L^2$ regularization
    \item Firing rate regularization (strength \& target Hz)
    \item Weight dropout rate
    \item Adam optimizer constants
    \item Number of layers
    \item Nodes per layer
    \item Input repeats
    \item Batch size
  \end{itemize}
  }	\renewcommand{\baselinestretch}{1}
\end{frame}


\begin{frame}{Hyperparameter sweep results}

	\begin{figure}[!ht]
		\includegraphics[clip, trim=0cm 0cm 0cm 2.5cm, width=0.64\linewidth]{scatters.pdf}  %lbrt
	\end{figure}

\end{frame}

\begin{frame}{}

	\begin{figure}[!ht]
		\includegraphics[clip, trim=0cm 0cm 0cm 0cm, width=0.6\linewidth]{all.pdf}  %lbrt
	\end{figure}

\end{frame}


\section{Next steps}
\begin{frame}{My questions}
\begin{itemize}[label=--]
\item Initial weights scaling? Freezing initial weights?
\item Thesis template? (\texttt{.tex})
\item Any questions or suggestions?
\item General evaluation of the project? Further course?
\end{itemize}


\end{frame}


\end{document}
