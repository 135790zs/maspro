
# MUST DO:
# TODO: Network model t's to Bellec
# TODO: Check ADAM
# TODO: Check weight update,
# TODO: Try edge cases
# TODO: MSE?
# TODO: Double check error term in softmax. Might interfere?
# TODO:

# LOW PRIORITY:
# TODO: Make it possible to use fewer than N_I N_Rs
# TODO: Make N_R variable per layer
# TODO: Experiment with reducing randomness a priori
# TODO: Only plot weights if tracking them
# TODO: Explore non-uniform synaptic delays
# TODO: Explore membrane potential reset
# TODO: Comment code

I can maybe use random B for all early layers and adaptive/symmetric B for last layer
ALSO note usage of W^out_kj in pE/pz equation in supp equation 20. Needs solution


=========

send plot to Jaeger, Dirk and Marco if working
may email Bellec et al if concrete question

=========
Freezing recurrent weights makes no difference. -> See if training with only recurrents works with very low learning rate. If so, I will need to have 2 different learning rates. Also explore if this is linked to Adam (see if also occurs using SGD).

=========

TODAY: 7 dec
# TODO: Finish introductory presentation
# TODO: Run best sweep settings for longer, try obtain good results
# TODO: Check maths
