
# TO DO (priority order):
	# Reproduce Bellec  (37.1 wrongs on test with Random)
		* Identify weakness.
	# Separate W_in and W_rec
	# Implement Izhikevich neurons (do these things while sweeping on mini nets and simplified data)

	# Implement matrix of distributed betas
	# Implement toggle for readout on last or all layers
	# Implement logging learning data (e.g. errs, LR, etc)
	# Sweep (intelligently, and with low-capacity), but only after reproducing
	# Obtain results
	  - One, two, three layers / readout on last or all layers (6 runs to sweep for)
	  - LSNN, ALIF, STDP-LIF, Izhikevich

	# Implement metaplasticity
	# Obtain results for metaplasticity
	# Refactor to track all run-based in own dict (etas, adam, weights, rates, metrics, etc). Subdict for train/val if applicable?
	# Read TODOs
	# Concatenate low-pass filter to input vectors?
	# L_reg can cumulative instead of list
	# A few things can be left out of the M[] dict
	# Make it possible to use fewer than N_I N_Rs
	# Make N_R variable per layer
	# Change into online learning rather than batch based.
	# Instead of is_ALIF, just make a mask of beta, since beta=0 means LIF
	# Fancy visualization (slider)
	# Refactor
	# Comment code

# TODO for thesis:
	# Read up on:
		- BCM rule and spike-rate dependent plasticity
		- Triplet-STDP
		  - First-spike dominating model (Wang2020b/41)
		  - Last-spike dominating model (Wang2020b/42)
		- paired-pulse facilitation or -depression.
		- Clopath rule
		- 3F Hebbian overview
	# Continue collecting facts from literature (In the 2020s, only Taherkani).
	# Draft clear and logical structure for report. Base it off my central research question, and top-down into introduction etc.

# IDEAS/NOTES
	# Thesis: in discussion: skip connections between layers? (sparse!)
	# Adam might be an excellent local training method, and could be integrated in the e-prop theory learning update in my paper.
	# Bellec uses std in normalize, why?

========

Correctly overfitting on small sets
- Requires some capacity -> maybe 2x400 isn't enough for full dataset
- Firing rate reg breaks performance at creg=50.
  - Leads to chaotic bursts
  - Increase refr instead?
  - Trains to bias mostly
  - creg of 1 same thing, and no good FR reg. Why?
  - May need to sweep over lower (0 to ~10) range. If all equal, over 0-30.
-


=========
Bellec reads out from all layers in multi-layer setup!!!!!!
Bellec's SGD actually uses momentum (of val 0.9).
=========

Global doesn't work well because of mean approx 0?
