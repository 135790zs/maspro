
# TO DO (priority order):
	# Reproduce Bellec  (37.1 wrongs on test)
		* Identify weakness.
		  - Are TZ's (and TZ_in) updating correctly?
		  - Double check everything. Use Bellec and actual numbers to manually recalculate.
		  - See if GPU works better now that np.wheres have been removed
	# Sweep (intelligently, and with low-capacity) only after reproducing
	# Implement optional L2 weight decay
	# Track all run-based in own dict (etas, adam, weights, rates, metrics, etc). Subdict for train/val if applicable?
	# Read TODOs
	# Vectorize as much as possible to speed up. Make sure to check for same functionality!
	# Implement Izhikevich neurons
	# Explore non-uniform synaptic delays
	# Explore membrane potential reset (Bellec Reset)
	# Concatenate low-pass filter to input vectors?
	# Make it possible to use fewer than N_I N_Rs
	# Make N_R variable per layer
	# Implement logging learning data (e.g. errs, LR, etc)
	# Change into online learning rather than batch based.
	# Fancy visualization (slider)
	# Refactor
	# Comment code

# TODO for thesis:
	# Read up on:
		- BCM rule and spike-rate dependent plasticity
		- Triplet-STDP
		  - First-spike dominating model (Wang2020b/41)
		  - Last-spike dominating model (Wang2020b/42)
		- paired-pulse facilitation or -depression.
		- Clopath rule
		- 3F Hebbian overview
	# Continue collecting facts from literature (In the 2020s, only Taherkani).
	# Draft clear and logical structure for report. Base it off my central research question, and top-down into introduction etc.

# IDEAS/NOTES
	# Gradient capping?
	# L2 weight decay
	# Different way to initialize model neurons (voltage etc?) Try eliminate onset chaos.

========

Correctly overfitting on small sets
- Requires some capacity -> maybe 2x400 isn't enough for full dataset
- Firing rate reg breaks performance at creg=50.
  - Leads to chaotic bursts
  - Increase refr instead?
  - Trains to bias mostly
  - creg of 1 same thing, and no good FR reg. Why?
  - May need to sweep over lower (0 to ~10) range. If all equal, over 0-30.
-


=========
Bellec reads out from all layers in multi-layer setup!!!!!!
Bellec's SGD actually uses momentum (of val 0.9).
=========

Global doesn't work well because of mean approx 0?
