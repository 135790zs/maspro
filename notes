
# MUST DO:
# TODO: LTP seems to work in vitro, see if traub leads to LTD

# LOW PRIORITY:
# TODO: Dictionary to facilitate sweeping function (param = key, list = item)
# TODO: Make it possible to use fewer than N_I N_Rs
# TODO: Make N_R variable per layer
# TODO: Maximize according to minmax in train
# TODO: Explore membrane potential reset
# TODO: Explore non-uniform synaptic delays
# TODO: Explore modifying the Pmax such that there is a "low-pass filter" of the previous Pmax


I can maybe use random B for all early layers and adaptive/symmetric B for last layer
ALSO note usage of W^out_kj in pE/pz equation in supp equation 20. Needs solution

Discussion point: online training, bidirectional training?

QUESTION FOR MEETING:
* for shallow layers?
* Bidirectional LSNN?
* Init weights to which range?


=========

send plot to Jaeger if working
may email Bellec et al if concrete question
