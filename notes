
# MUST DO:
# TODO: Visualize and thereby validate batch-based system. Refactor as needed.
# TODO: Implement TIMIT
# TODO: Implement L2 reg: Add 1e-5 * ||W||^2 to loss
# TODO: Add firing rate regularization with c_reg = 50. See supplementary notes.
# TODO: Gaussian distribution for broadcast weights (mu=0, var=1)
# TODO: Implement ADAM
# TODO: Use minibatch-based gradient computation (does this mean updating after 32 samples?)
# TODO: LTP seems to work in vitro, see if traub leads to LTD

# LOW PRIORITY:
# TODO: Dictionary to facilitate sweeping function (param = key, list = item)

=========================================================

MEETING NOTES 10/13

* don't do too much, but do it right
  zwaartekracht NWO


* homeostaticity, slower dynamics in deeper layers. Neural sampling.
* Fading input not necessarily bad: think about it
* Learn nonlinear filter?
* nonlinear autoregressive moving average NARMA
* Input: drop autoregressive
* Search for NARMA-10 benchmark
* SEND SKYPE NAME

=========================================================

MEETING NOTES 24/13

* N/A

---------------------------------------------------------
SCRATCHPAD

* make new file to process TIMIT files and pipeline the train/val process

For 80 epochs:
	For each minibatch of subsequent 32 audio frames:
		Feed each audio frame to the network for 5ms
	Test on validation set, save network parameters iff lowest error

Load network with lowest validation error

------------
